{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Embedding\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) systems are gaining significant popularity among legacy industries such as law, life sciences, and finance, where there are massive amounts of unstructured text that are multimodal. Gathering insights from these extensive piles of documentation previously involved manual searches and insight generation from graphs and diagrams, which are extremely time-consuming and laborious, even for highly intelligent individuals. Companies like Harvey and Hebbia that have recently bagged huge funds exemplify how RAG systems can expedite this process by not only finding relevant documents but also providing a GPT-like interface that directly answers user questions.\n",
    "\n",
    "However, RAG systems often hallucinate, especially when they fail to find relevant answers from the pool of embedded documents. Achieving performance from 80% to 100% is extremely challenging but crucial especially for vertical use cases where mistakes can be costly and unforgiving.\n",
    "\n",
    "While foundational models are often blamed and guardrails built with hallucination models (e.g., [Lynx](https://www.patronus.ai/blog/lynx-state-of-the-art-open-source-hallucination-detection-model)) are gaining popularity, the importance of embedding strategies and the limitations of multimodal embedding are less frequently discussed.\n",
    "\n",
    "**Agentic Embedding** is a new AI engineering term that I coined, which implies a method of utilizing different prompts or methods to embed various types of modalities (e.g., text, tables, graphs, diagrams, photos, etc.). While the code serves as a simple demonstration of the concept, it also explores the current limitations of traditional OCR methods in processing unstructured multimodal documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're using the Mistral 7B research paper from arXiv: [https://arxiv.org/abs/2310.06825](https://arxiv.org/abs/2310.06825). This paper has been selected for demonstration because it's truly multimodal! (including graphs, bar charts, and diagrams).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Page:\n",
    "\n",
    "![Alt text](./files/sample.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-core\n",
    "!pip install llama-index-llms-anthropic llama-index-multi-modal-llms-anthropic\n",
    "!pip install llama-index-embeddings-huggingface\n",
    "!pip install llama-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-embeddings-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LlamaParse](https://github.com/run-llama/llama_parse) is a genAi native document parsing tool. \n",
    "\n",
    "First, we parse the pdf into json that includes a markdown version of the text (useful for tabular data extraction) and also image which are present in the json in the form of ImageNode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from llama_parse import LlamaParse \n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "# API access to llama-cloud\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = LlamaParse(verbose=True)\n",
    "file_path = \"json_objs.json\"\n",
    "\n",
    "try:\n",
    "    # Try to load the JSON file\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        json_objs = json.load(json_file)\n",
    "    print(\"Loaded json_objs from file.\")\n",
    "except FileNotFoundError:\n",
    "    # If the file does not exist, run the parser and save the result\n",
    "    json_objs = parser.get_json_result(\"./files/Mistral_7B.pdf\")\n",
    "    \n",
    "    # Save the json_objs to a file for future use\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(json_objs, json_file)\n",
    "    print(\"File not found. Parsed the PDF and saved the result.\")\n",
    "\n",
    "json_list = json_objs[0][\"pages\"]\n",
    "json_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations of OCR / current document parsing\n",
    "The cell below shows that llamaParse failed to extract the image from page 6. And I found that this is where most document parsers fail. Charts that are embedded as Vector Graphics are not stored as one cohesive unit within the pdf structure making it hard to parse. The limitations are significant as the format of images, charts, and tables vastly differ within and across documents. An ideal extraction would be vision-based image localization approach where the figure and it's legends are both extracted as a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separately save images into the extracted_images directory\n",
    "images = parser.get_images(json_objs, download_path='extracted_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform embedding augmentation with images using claude agents\n",
    "from IPython.display import Image\n",
    "from anthropic import Anthropic\n",
    "import base64\n",
    "import pprint\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "\n",
    "client = Anthropic(api_key=\"sk-...\")\n",
    "MODEL_NAME = \"claude-3-opus-20240229\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining embedding tools\n",
    "Here, I have defined two tools: one for diagrams and one for graphs. In this example, it's an extremely simple agent that performs classification and data extraction depending on the type of image provided. However, a more complex method could potentially be implemented inside `process_tool_call`, such as adding metadata for pre-filtering based on the user query, depending on the specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools\n",
    "image_label_tool_generic = {\n",
    "    \"name\": \"print_research_image_info\",\n",
    "    \"description\": \"Extracts useful image information from a research paper.\",\n",
    "    \"input_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"string\", \"description\": \"Title of the figure.\"},\n",
    "            \"legend\": {\"type\": \"string\", \"description\": \"Legend of the figure.\"},\n",
    "            \"description\": {\"type\": \"string\", \"description\": \"Description of the image.\"},\n",
    "            \"keywords\": {\"type\": \"string\", \"description\": \"Several specific keywords that describ the image.\"},\n",
    "            \"x-axis\" : {\"type\": \"string\", \"description\": \"X axis of the graph\"},\n",
    "            \"y-axis\" : {\"type\": \"string\", \"description\": \"Y axis of the graph\"}\n",
    "        },\n",
    "        \"required\": [\"title\", \"legend\", \"description\", \"keywords\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define specific tools\n",
    "image_label_tool_diagram = {\n",
    "    \"name\": \"get_research_diagram_info\",\n",
    "    \"description\": \"Interprets diagram information from a research paper.\",\n",
    "    \"input_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"string\", \"description\": \"Title of the figure.\"},\n",
    "            \"legend\": {\"type\": \"string\", \"description\": \"Legend of the figure.\"},\n",
    "            \"description\": {\"type\": \"string\", \"description\": \"Description of the diagram.\"},\n",
    "            \"keywords\": {\"type\": \"string\", \"description\": \"Several specific keywords that describ the diagram.\"}\n",
    "        },\n",
    "        \"required\": [\"titke\", \"legend\", \"description\", \"keywords\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "image_label_tool_graph = {\n",
    "    \"name\": \"get_research_graph_info\",\n",
    "    \"description\": \"Interprets graph information from a research paper.\",\n",
    "    \"input_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"string\", \"description\": \"Title of the figure.\"},\n",
    "            \"legend\": {\"type\": \"string\", \"description\": \"Legend of the figure.\"},\n",
    "            \"description\": {\"type\": \"string\", \"description\": \"Description of the image\"},\n",
    "            \"keywords\": {\"type\": \"string\", \"description\": \"Several specific keywords that describes the diagram.\"},\n",
    "            \"trend\": {\"type\": \"string\", \"description\": \"Overall trend of the graph.\"},\n",
    "            \"x-axis\" : {\"type\": \"string\", \"description\": \"X axis of the graph\"},\n",
    "            \"y-axis\" : {\"type\": \"string\", \"description\": \"Y axis of the graph\"}\n",
    "        },\n",
    "        \"required\": [\"title\", \"legend\", \"description\", \"keywords\",'trend', \"x-axis\", \"y-axis\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [image_label_tool_diagram, image_label_tool_graph]\n",
    "\n",
    "\n",
    "def process_tool_call(tool_name, tool_input, image_path): \n",
    "    if(tool_name == 'get_research_diagram_info'):\n",
    "        # Relevant embedding function here with metadata\n",
    "        return TextNode(text=str(tool_input), metadata={\"path\": image_path, \"type\": 'diagram'})\n",
    "\n",
    "    elif(tool_name == \"get_research_graph_info\"):\n",
    "        return TextNode(text=str(tool_input), metadata={\"path\": image_path, \"type\": \"graph\"})\n",
    "\n",
    "\n",
    "#..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emcode image to pass it to the LLM\n",
    "def get_base64_encoded_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        binary_data = image_file.read()\n",
    "        base_64_encoded_data = base64.b64encode(binary_data)\n",
    "        base64_string = base_64_encoded_data.decode('utf-8')\n",
    "        return base64_string\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context Augmentation\n",
    "\n",
    "Providing context to the image before embedding, massively helps Vlm's capability to interpret charts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store everything into text_nodes and image_text_nodes\n",
    "# For every image in the llama parse, provide context\n",
    "def agentic_embedding(context, image_path):\n",
    "\n",
    "    query = f'Print the description of the image provided from a research paper. Provided is the context of the image in markdown format: {context}.'\n",
    "    message_list = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": get_base64_encoded_image(image_path)}},\n",
    "                {\"type\": \"text\", \"text\": query}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=4096,\n",
    "        messages=message_list,\n",
    "        tools=tools\n",
    "    )\n",
    "\n",
    "    if response.stop_reason == \"tool_use\":\n",
    "        last_content_block = response.content[-1]\n",
    "        if last_content_block.type == 'tool_use':\n",
    "            tool_name = last_content_block.name\n",
    "            tool_inputs = last_content_block.input\n",
    "            print(f\"\\nTool Used: {tool_name}\")\n",
    "            pp.pprint(f\"Tool Input: {tool_inputs}\")\n",
    "\n",
    "            return process_tool_call(tool_name, tool_inputs, image_path)\n",
    "\n",
    "image_text_nodes = []\n",
    "for image in images:\n",
    "    image_path = image['path']\n",
    "    page_number = image['page_number']\n",
    "    context = json_list[page_number - 1]['md']\n",
    "    \n",
    "    image_text_node = agentic_embedding(context, image_path)\n",
    "    image_text_nodes.append(image_text_node)\n",
    "    print(\"image text node:\", image_text_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "llm = Anthropic(model=\"claude-3-opus-20240229\", temperature=0.0, api_key=\"sk...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "\n",
    "Settings.llm = llm\n",
    "# Settings.embed_model = \"local:BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(api_key=\"sk...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextNode(id_='0eab90d1-8ce2-4ad4-8a4a-a74057b400ca', embedding=None, metadata={'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p1_1.png', 'type': 'diagram'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"{'title': 'Sliding Window Attention', 'legend': 'The matrices visualize attention patterns across transformer layers. Rows represent layers and columns represent token positions. Yellow indicates where attention is applied within the sliding window for each layer.', 'description': 'The diagram compares vanilla attention to sliding window attention across transformer layers. With vanilla attention, each token attends to all previous tokens. Sliding window attention restricts the attention to a fixed window that shifts with each layer. This allows tokens in higher layers to indirectly attend to information beyond the initial window, increasing the effective context length captured by the model as you move up the layer stack.', 'keywords': 'sliding window attention, transformer layers, effective context length, attention patterns'}\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='82f418df-8fc3-4a70-800f-f9c76a1cd89b', embedding=None, metadata={'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p2_1.png', 'type': 'diagram'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"{'title': 'Rolling buffer cache', 'legend': 'The hidden state corresponding to the latest generated tokens are colored in orange.', 'description': 'This diagram illustrates how a rolling buffer cache works in a language model over multiple timesteps. The cache has a fixed size (4 tokens in this example) and rolls over, overwriting old token values as new ones are generated and added. The hidden states corresponding to the most recently generated tokens at each timestep are highlighted in orange.', 'keywords': 'rolling buffer, cache, language model, overwrite, hidden states'}\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='be167ebb-cbaa-4367-bcaa-a8bc8782aaf8', embedding=None, metadata={'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p3_1.png', 'type': 'graph'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"{'title': 'Performance Comparison of Mistral 7B and Llama 2 Models', 'legend': 'Mistral 7B, LLaMA 2 13B, LLaMA 2 7B, LLaMA 1 34B', 'description': 'The graph compares the accuracy scores of the Mistral 7B model against different sized Llama 2 models on 8 benchmark tasks. In general, Mistral 7B outperforms the Llama models, especially on reasoning, comprehension, math and code tasks. The performance gap is widest on the Code benchmark.', 'keywords': 'NLP model comparison, benchmark performance, Mistral 7B, Llama 2 models, reasoning, comprehension, math, code', 'trend': 'Mistral 7B has the highest accuracy scores across all benchmarks, with its lead most significant on reasoning, comprehension and coding tasks.', 'x-axis': 'Benchmark tasks - MMLU, Knowledge, Reasoning, Comprehension, AGI Eval, Math, BBH, Code', 'y-axis': 'Accuracy percentage scores from 30% to 70%'}\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='095f008f-6a08-4b8b-b57b-3b141063abed', embedding=None, metadata={'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p4_1.png', 'type': 'graph'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"{'title': 'LLaMA 2 vs Mistral performance across model sizes', 'legend': 'LLaMA 2, Mistral', 'description': 'The graphs compare the performance of LLaMA 2 and Mistral language models on various benchmarks like MMLU, commonsense reasoning, world knowledge and reading comprehension at different model sizes ranging from 7B to 70B parameters.', 'keywords': 'MMLU, commonsense reasoning, world knowledge, reading comprehension, model size', 'trend': 'The Mistral 7B model largely outperforms LLaMA 2 13B on all the benchmarks except knowledge, where they are on par. This is likely due to the limited parameter count of Mistral 7B limiting the amount of knowledge it can compress compared to larger models.', 'x-axis': 'Model size (billion parameters)', 'y-axis': 'Performance metrics - MMLU %, Reasoning %, Knowledge %, Comprehension %'}\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='7c0405f7-8bf2-4e05-acdc-500bc3f1a0a5', embedding=None, metadata={'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p6_1.png', 'type': 'graph'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"{'title': 'Human evaluation of Mistral 7B – Instruct vs Llama 2 13B – Chat Example', 'legend': ' ', 'description': 'An example of human evaluation comparing Llama 2 13B – Chat and Mistral 7B – Instruct on a question asking for book recommendations on quantum physics. Llama 2 13B – Chat recommends a general physics book, while Mistral 7B – Instruct recommends a more relevant book specifically on quantum physics and describes the contents in more detail.', 'keywords': 'human evaluation, Mistral 7B, Llama 2 13B, book recommendation, quantum physics', 'trend': 'Mistral 7b provides a more relevant and detailed book recommendation compared to Llama 2 13b', 'x-axis': 'The two language models being compared - Llama 2 13b and Mistral 7b', 'y-axis': ' '}\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_image_text_node = [x for x in image_text_nodes if x is not None]\n",
    "filtered_image_text_node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed these\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Here, we're just embedding images for the purpose of the project\n",
    "# Also for better embedding and to have more control over, you can use pipelining from Llama index\n",
    "# https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\n",
    "# index = VectorStoreIndex(text_nodes + image_text_nodes)\n",
    "index = VectorStoreIndex(filtered_image_text_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the performance comparison graphs, the Mistral 7B model would likely outperform the LLaMA 2 models of various sizes on the MMLU benchmark. The first graph shows Mistral 7B achieving a higher accuracy score than the LLaMA 2 models, including the larger 13B parameter version, on the MMLU task.\n",
      "\n",
      "The second set of graphs also indicates that Mistral 7B has superior performance to LLaMA 2 13B on the MMLU benchmark despite having fewer parameters. This suggests that the Mistral 7B architecture and training allow it to be more efficient and effective at the MMLU task compared to the LLaMA 2 models.\n",
      "\n",
      "So in summary, the Mistral 7B model would be expected to outperform LLaMA 2 models, even those with more parameters like the 13B version, when evaluated on the MMLU benchmark based on the performance data shown.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How would mistral 7B perform against other Llama models in the MMLU benchmark?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Next\n",
    "\n",
    "There are two major limitations that hinder the current OCR/Document Parser + VLM embedding:\n",
    "\n",
    "1. **Speed of Embedding**: The process of embedding can be time-consuming, affecting the overall efficiency.\n",
    "2. **Dirty Output Format**: Current data extraction tools often produce unclean outputs, particularly for automatic schema inference tasks with multimodal documentation.\n",
    "\n",
    "The ideal solution would involve extracting images as a whole, including their legends, and also being able to extract vector graphic images from PDFs, which current parsing tools fail to do. It is surprising how much the formats of these multimodal documents vary, making it nearly impossible to enforce consistent rules on these parsers to make them work.\n",
    "\n",
    "This makes the new [Colpali](https://huggingface.co/blog/manu/colpali) model an interesting option to explore. Colpali takes a complete vision approach to embedding unstructured data by solely using the image representation of document pages.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
