{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Embedding\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) systems are gaining significant popularity among legacy industries such as law, life sciences, and finance, where there are massive amounts of unstructured text that are multimodal. Gathering insights from these extensive piles of documentation previously involved manual searches and insight generation from graphs and diagrams, which are extremely time-consuming and laborious, even for highly intelligent individuals. Companies like Harvey and Hebbia that have recently bagged huge funds exemplify how RAG systems can expedite this process by not only finding relevant documents but also providing a GPT-like interface that directly answers user questions.\n",
    "\n",
    "However, RAG systems often hallucinate, especially when they fail to find relevant answers from the pool of embedded documents. Achieving performance from 80% to 100% is extremely challenging but crucial especially for vertical use cases where mistakes can be costly and unforgiving.\n",
    "\n",
    "While foundational models are often blamed and guardrails built with hallucination models (e.g., [Lynx](https://www.patronus.ai/blog/lynx-state-of-the-art-open-source-hallucination-detection-model)) are gaining popularity, the importance of embedding strategies and the limitations of multimodal embedding are less frequently discussed.\n",
    "\n",
    "**Agentic Embedding** is a new AI engineering term that I coined, which implies a method of utilizing different prompts or methods to embed various types of modalities (e.g., text, tables, graphs, diagrams, photos, etc.). While the code serves as a simple demonstration of the concept, it also explores the current limitations of traditional OCR methods in processing unstructured multimodal documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're using the Mistral 7B research paper from arXiv: [https://arxiv.org/abs/2310.06825](https://arxiv.org/abs/2310.06825). This paper has been selected for demonstration because it's truly multimodal! (including graphs, bar charts, and diagrams).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Page:\n",
    "\n",
    "![Alt text](files/sample.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting llama-index\n",
      "  Downloading llama_index-0.10.58-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index)\n",
      "  Downloading llama_index_agent_openai-0.2.9-py3-none-any.whl.metadata (729 bytes)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_cli-0.1.13-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core==0.10.58 (from llama-index)\n",
      "  Downloading llama_index_core-0.10.58-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n",
      "  Using cached llama_index_embeddings_openai-0.1.11-py3-none-any.whl.metadata (655 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.2.7-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
      "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.27 (from llama-index)\n",
      "  Downloading llama_index_llms_openai-0.1.27-py3-none-any.whl.metadata (610 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.1.8-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n",
      "  Downloading llama_index_program_openai-0.1.7-py3-none-any.whl.metadata (760 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n",
      "  Downloading llama_index_readers_file-0.1.31-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.58->llama-index) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (2024.6.1)\n",
      "Requirement already satisfied: httpx in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (3.8.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (1.24.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (1.37.0)\n",
      "Requirement already satisfied: pandas in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (2.0.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (10.4.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core==0.10.58->llama-index) (1.16.0)\n",
      "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index)\n",
      "  Downloading llama_cloud-0.0.11-py3-none-any.whl.metadata (751 bytes)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: llama-parse>=0.4.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-readers-llama-parse>=0.1.2->llama-index) (0.4.9)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.58->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.58->llama-index) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.58->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.58->llama-index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.58->llama-index) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.58->llama-index) (4.0.3)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: pydantic>=1.10 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (2.8.2)\n",
      "Requirement already satisfied: anyio in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpx->llama-index-core==0.10.58->llama-index) (4.4.0)\n",
      "Requirement already satisfied: certifi in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpx->llama-index-core==0.10.58->llama-index) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpx->llama-index-core==0.10.58->llama-index) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpx->llama-index-core==0.10.58->llama-index) (3.7)\n",
      "Requirement already satisfied: sniffio in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpx->llama-index-core==0.10.58->llama-index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpcore==1.*->httpx->llama-index-core==0.10.58->llama-index) (0.14.0)\n",
      "Requirement already satisfied: click in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.58->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.58->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.58->llama-index) (2024.5.15)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from openai>=1.1.0->llama-index-core==0.10.58->llama-index) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from requests>=2.31.0->llama-index-core==0.10.58->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from requests>=2.31.0->llama-index-core==0.10.58->llama-index) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.58->llama-index) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.58->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from dataclasses-json->llama-index-core==0.10.58->llama-index) (3.21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pandas->llama-index-core==0.10.58->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pandas->llama-index-core==0.10.58->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pandas->llama-index-core==0.10.58->llama-index) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from anyio->httpx->llama-index-core==0.10.58->llama-index) (1.2.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.10.58->llama-index) (21.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pydantic>=1.10->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pydantic>=1.10->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.58->llama-index) (1.15.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.10.58->llama-index) (3.0.8)\n",
      "Downloading llama_index-0.10.58-py3-none-any.whl (6.8 kB)\n",
      "Downloading llama_index_core-0.10.58-py3-none-any.whl (15.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m923.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_agent_openai-0.2.9-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_cli-0.1.13-py3-none-any.whl (27 kB)\n",
      "Using cached llama_index_embeddings_openai-0.1.11-py3-none-any.whl (6.3 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.2.7-py3-none-any.whl (9.5 kB)\n",
      "Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading llama_index_llms_openai-0.1.27-py3-none-any.whl (11 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.1.8-py3-none-any.whl (5.9 kB)\n",
      "Downloading llama_index_program_openai-0.1.7-py3-none-any.whl (5.3 kB)\n",
      "Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.1.31-py3-none-any.whl (38 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_cloud-0.0.11-py3-none-any.whl (154 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m533.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m679.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: striprtf, soupsieve, pypdf, beautifulsoup4, llama-cloud, llama-index-legacy, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "  Attempting uninstall: llama-index-core\n",
      "    Found existing installation: llama-index-core 0.10.57\n",
      "    Uninstalling llama-index-core-0.10.57:\n",
      "      Successfully uninstalled llama-index-core-0.10.57\n",
      "Successfully installed beautifulsoup4-4.12.3 llama-cloud-0.0.11 llama-index-0.10.58 llama-index-agent-openai-0.2.9 llama-index-cli-0.1.13 llama-index-core-0.10.58 llama-index-embeddings-openai-0.1.11 llama-index-indices-managed-llama-cloud-0.2.7 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.27 llama-index-multi-modal-llms-openai-0.1.8 llama-index-program-openai-0.1.7 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.31 llama-index-readers-llama-parse-0.1.6 pypdf-4.3.1 soupsieve-2.5 striprtf-0.0.26\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-index-core in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (0.10.58)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (2024.6.1)\n",
      "Requirement already satisfied: httpx in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (3.8.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (1.24.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (1.37.0)\n",
      "Requirement already satisfied: pandas in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (2.0.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (10.4.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (4.0.3)\n",
      "Requirement already satisfied: click in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core) (2024.5.15)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from openai>=1.1.0->llama-index-core) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from openai>=1.1.0->llama-index-core) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from openai>=1.1.0->llama-index-core) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from openai>=1.1.0->llama-index-core) (1.3.1)\n",
      "Requirement already satisfied: certifi in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpx->llama-index-core) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpx->llama-index-core) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpx->llama-index-core) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from requests>=2.31.0->llama-index-core) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from requests>=2.31.0->llama-index-core) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from typing-inspect>=0.8.0->llama-index-core) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from dataclasses-json->llama-index-core) (3.21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pandas->llama-index-core) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pandas->llama-index-core) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pandas->llama-index-core) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-core) (1.2.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (21.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core) (1.15.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (3.0.8)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting llama-index-llms-anthropic\n",
      "  Downloading llama_index_llms_anthropic-0.1.16-py3-none-any.whl.metadata (691 bytes)\n",
      "Collecting llama-index-multi-modal-llms-anthropic\n",
      "  Downloading llama_index_multi_modal_llms_anthropic-0.1.6-py3-none-any.whl.metadata (725 bytes)\n",
      "Collecting anthropic<0.29.0,>=0.26.2 (from llama-index-llms-anthropic)\n",
      "  Downloading anthropic-0.28.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.57 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-llms-anthropic) (0.10.58)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic) (1.3.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic) (0.19.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (2024.6.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (3.8.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (1.24.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (1.37.0)\n",
      "Requirement already satisfied: pandas in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (2.0.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (10.4.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (4.66.4)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (4.0.3)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from anyio<5,>=3.5.0->anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from anyio<5,>=3.5.0->anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic) (1.2.2)\n",
      "Requirement already satisfied: certifi in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpx<1,>=0.23.0->anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpx<1,>=0.23.0->anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic) (0.14.0)\n",
      "Requirement already satisfied: click in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (2024.5.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pydantic<3,>=1.9.0->anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pydantic<3,>=1.9.0->anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (3.0.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from tokenizers>=0.13.0->anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic) (0.24.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (3.21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (2024.1)\n",
      "Requirement already satisfied: filelock in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic) (3.15.4)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-anthropic) (1.15.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from packaging>=20.9->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic) (3.0.8)\n",
      "Downloading llama_index_llms_anthropic-0.1.16-py3-none-any.whl (6.5 kB)\n",
      "Downloading llama_index_multi_modal_llms_anthropic-0.1.6-py3-none-any.whl (5.8 kB)\n",
      "Downloading anthropic-0.28.1-py3-none-any.whl (862 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.7/862.7 kB\u001b[0m \u001b[31m562.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: anthropic, llama-index-multi-modal-llms-anthropic, llama-index-llms-anthropic\n",
      "  Attempting uninstall: anthropic\n",
      "    Found existing installation: anthropic 0.31.2\n",
      "    Uninstalling anthropic-0.31.2:\n",
      "      Successfully uninstalled anthropic-0.31.2\n",
      "Successfully installed anthropic-0.28.1 llama-index-llms-anthropic-0.1.16 llama-index-multi-modal-llms-anthropic-0.1.6\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting llama-index-embeddings-huggingface\n",
      "  Downloading llama_index_embeddings_huggingface-0.2.2-py3-none-any.whl.metadata (769 bytes)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.24.2)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-embeddings-huggingface) (0.10.58)\n",
      "Collecting sentence-transformers>=2.6.1 (from llama-index-embeddings-huggingface)\n",
      "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.12.2)\n",
      "Requirement already satisfied: aiohttp in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.9.5)\n",
      "Collecting minijinja>=1.0 (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading minijinja-2.0.1-cp38-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.0.31)\n",
      "Requirement already satisfied: dataclasses-json in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.8)\n",
      "Requirement already satisfied: httpx in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.8.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.24.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.37.0)\n",
      "Requirement already satisfied: pandas in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.0.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (10.4.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.7.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.16.0)\n",
      "Collecting transformers<5.0.0,>=4.34.0 (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m408.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch>=1.11.0 (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading torch-2.2.2-cp38-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting scikit-learn (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-macosx_10_9_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading scipy-1.10.1-cp38-cp38-macosx_10_9_x86_64.whl.metadata (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m652.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.0.3)\n",
      "Requirement already satisfied: click in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.5.15)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.3.1)\n",
      "Requirement already satisfied: certifi in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.14.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from packaging>=20.9->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.0.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.0.3)\n",
      "Collecting sympy (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.34.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading safetensors-0.4.3-cp38-cp38-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.19.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.1)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.2.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.15.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading MarkupSafe-2.1.5-cp38-cp38-macosx_10_9_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading llama_index_embeddings_huggingface-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m193.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading minijinja-2.0.1-cp38-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m200.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.2-cp38-none-macosx_10_9_x86_64.whl (150.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0mm\n",
      "\u001b[?25hDownloading transformers-4.43.3-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.3.2-cp38-cp38-macosx_10_9_x86_64.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m623.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.10.1-cp38-cp38-macosx_10_9_x86_64.whl (35.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp38-cp38-macosx_10_12_x86_64.whl (416 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.3/416.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m555.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.5-cp38-cp38-macosx_10_9_x86_64.whl (14 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: mpmath, threadpoolctl, sympy, scipy, safetensors, minijinja, MarkupSafe, scikit-learn, jinja2, torch, transformers, sentence-transformers, llama-index-embeddings-huggingface\n",
      "Successfully installed MarkupSafe-2.1.5 jinja2-3.1.4 llama-index-embeddings-huggingface-0.2.2 minijinja-2.0.1 mpmath-1.3.0 safetensors-0.4.3 scikit-learn-1.3.2 scipy-1.10.1 sentence-transformers-3.0.1 sympy-1.13.1 threadpoolctl-3.5.0 torch-2.2.2 transformers-4.43.3\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-parse in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (0.4.9)\n",
      "Requirement already satisfied: llama-index-core>=0.10.29 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-parse) (0.10.58)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core>=0.10.29->llama-parse) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (2024.6.1)\n",
      "Requirement already satisfied: httpx in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (3.8.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (1.24.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (1.37.0)\n",
      "Requirement already satisfied: pandas in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (2.0.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (10.4.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core>=0.10.29->llama-parse) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.10.29->llama-parse) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.10.29->llama-parse) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.10.29->llama-parse) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.10.29->llama-parse) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.10.29->llama-parse) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.10.29->llama-parse) (4.0.3)\n",
      "Requirement already satisfied: click in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core>=0.10.29->llama-parse) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core>=0.10.29->llama-parse) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core>=0.10.29->llama-parse) (2024.5.15)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from openai>=1.1.0->llama-index-core>=0.10.29->llama-parse) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from openai>=1.1.0->llama-index-core>=0.10.29->llama-parse) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from openai>=1.1.0->llama-index-core>=0.10.29->llama-parse) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from openai>=1.1.0->llama-index-core>=0.10.29->llama-parse) (1.3.1)\n",
      "Requirement already satisfied: certifi in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpx->llama-index-core>=0.10.29->llama-parse) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpx->llama-index-core>=0.10.29->llama-parse) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpx->llama-index-core>=0.10.29->llama-parse) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpcore==1.*->httpx->llama-index-core>=0.10.29->llama-parse) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from requests>=2.31.0->llama-index-core>=0.10.29->llama-parse) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from requests>=2.31.0->llama-index-core>=0.10.29->llama-parse) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core>=0.10.29->llama-parse) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from typing-inspect>=0.8.0->llama-index-core>=0.10.29->llama-parse) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from dataclasses-json->llama-index-core>=0.10.29->llama-parse) (3.21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pandas->llama-index-core>=0.10.29->llama-parse) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pandas->llama-index-core>=0.10.29->llama-parse) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pandas->llama-index-core>=0.10.29->llama-parse) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-core>=0.10.29->llama-parse) (1.2.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core>=0.10.29->llama-parse) (21.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core>=0.10.29->llama-parse) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core>=0.10.29->llama-parse) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core>=0.10.29->llama-parse) (1.15.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core>=0.10.29->llama-parse) (3.0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-core\n",
    "!pip install llama-index-llms-anthropic llama-index-multi-modal-llms-anthropic\n",
    "!pip install llama-index-embeddings-huggingface\n",
    "!pip install llama-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-embeddings-openai\n",
      "  Downloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl.metadata (655 bytes)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-embeddings-openai) (0.10.57)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2024.6.1)\n",
      "Requirement already satisfied: httpx in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.8.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.24.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.37.0)\n",
      "Requirement already satisfied: pandas in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2.0.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (10.4.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (4.0.3)\n",
      "Requirement already satisfied: click in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2024.5.15)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.3.1)\n",
      "Requirement already satisfied: certifi in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.2.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (21.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.15.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/kim-unyoung/Library/Python/3.8/lib/python/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.0.8)\n",
      "Downloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl (6.3 kB)\n",
      "Installing collected packages: llama-index-embeddings-openai\n",
      "Successfully installed llama-index-embeddings-openai-0.1.11\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.8 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-embeddings-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LlamaParse](https://github.com/run-llama/llama_parse) is a genAi native document parsing tool. \n",
    "\n",
    "First, we parse the pdf into json that includes a markdown version of the text (useful for tabular data extraction) and also image which are present in the json in the form of ImageNode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from llama_parse import LlamaParse \n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "# API access to llama-cloud\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded json_objs from file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'page': 1,\n",
       "  'text': 'arXiv:2310.06825v1 [cs.CL] 10 Oct 2023\\n\\n                                                                               Mistral 7B\\n\\n                                            Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\n                                        Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\n                                       Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\n                                         Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\n                                                                                William El Sayed\\n                                             MsuA\\n                                                                                    Abstract\\n\\n                                          We introduce Mistral 7B, a 7–billion-parameter language model engineered for\\n                                          superior performance and efficiency. Mistral 7B outperforms the best open 13B\\n                                          model (Llama 2) across all evaluated benchmarks, and the best released 34B\\n                                          model (Llama 1) in reasoning, mathematics, and code generation. Our model\\n                                          leverages grouped-query attention (GQA) for faster inference, coupled with sliding\\n                                          window attention (SWA) to effectively handle sequences of arbitrary length with a\\n                                          reduced inference cost. We also provide a model fine-tuned to follow instructions,\\n                                          Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\n                                          automated benchmarks. Our models are released under the Apache 2.0 license.\\n                                          Code: https://github.com/mistralai/mistral-src\\n                                          Webpage: https://mistral.ai/news/announcing-mistral-7b/\\n\\n                                1    Introduction\\n                                In the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model\\n                                performance often necessitates an escalation in model size. However, this scaling tends to increase\\n                                computational costs and inference latency, thereby raising barriers to deployment in practical,\\n                                real-world scenarios. In this context, the search for balanced models delivering both high-level\\n                                performance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that\\n                                a carefully designed language model can deliver high performance while maintaining an efficient\\n                                inference. Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested\\n                                benchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25 ]) in mathematics and code\\n                                generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20 ],\\n                                without sacrificing performance on non-code related benchmarks.\\n                                Mistral 7B leverages grouped-query attention (GQA) [ 1 ], and sliding window attention (SWA) [6, 3].\\n                                GQA significantly accelerates the inference speed, and also reduces the memory requirement during\\n                                decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\\n                                applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\\n                                computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\\n                                collectively contribute to the enhanced performance and efficiency of Mistral 7B.',\n",
       "  'md': '# arXiv:2310.06825v1 [cs.CL] 10 Oct 2023\\n\\nMistral 7BAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El SayedAbstractWe introduce Mistral 7B, a 7–billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/announcing-mistral-7b/\\n\\n# Introduction\\n\\nIn the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model performance often necessitates an escalation in model size. However, this scaling tends to increase computational costs and inference latency, thereby raising barriers to deployment in practical, real-world scenarios. In this context, the search for balanced models delivering both high-level performance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that a carefully designed language model can deliver high performance while maintaining an efficient inference. Mistral 7B outperforms the previous best 13B model (Llama 2, [26]) across all tested benchmarks, and surpasses the best 34B model (LLaMa 34B, [25]) in mathematics and code generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [20], without sacrificing performance on non-code related benchmarks. Mistral 7B leverages grouped-query attention (GQA) [1], and sliding window attention (SWA) [6, 3]. GQA significantly accelerates the inference speed, and also reduces the memory requirement during decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms collectively contribute to the enhanced performance and efficiency of Mistral 7B.',\n",
       "  'images': [{'name': 'img_p0_1.png',\n",
       "    'height': 214,\n",
       "    'width': 641,\n",
       "    'x': 147.599,\n",
       "    'y': 218.045,\n",
       "    'original_width': 1500,\n",
       "    'original_height': 500,\n",
       "    'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p0_1.png',\n",
       "    'job_id': '12810641-4017-4eb7-9ac3-4129c2421dbc',\n",
       "    'original_pdf_path': './files/Mistral_7B.pdf',\n",
       "    'page_number': 1}],\n",
       "  'items': [{'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'arXiv:2310.06825v1 [cs.CL] 10 Oct 2023',\n",
       "    'md': '# arXiv:2310.06825v1 [cs.CL] 10 Oct 2023',\n",
       "    'bBox': {'x': None, 'y': 32, 'w': 345.5200000000001, 'h': 20}},\n",
       "   {'type': 'text',\n",
       "    'value': 'Mistral 7BAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El SayedAbstractWe introduce Mistral 7B, a 7–billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/announcing-mistral-7b/',\n",
       "    'md': 'Mistral 7BAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El SayedAbstractWe introduce Mistral 7B, a 7–billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/announcing-mistral-7b/',\n",
       "    'bBox': {'x': 133, 'y': 114, 'w': 344.1380918, 'h': 17.2154}},\n",
       "   {'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Introduction',\n",
       "    'md': '# Introduction',\n",
       "    'bBox': {'x': 125, 'y': 523, 'w': 64.8808704, 'h': 11.9552}},\n",
       "   {'type': 'text',\n",
       "    'value': 'In the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model performance often necessitates an escalation in model size. However, this scaling tends to increase computational costs and inference latency, thereby raising barriers to deployment in practical, real-world scenarios. In this context, the search for balanced models delivering both high-level performance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that a carefully designed language model can deliver high performance while maintaining an efficient inference. Mistral 7B outperforms the previous best 13B model (Llama 2, [26]) across all tested benchmarks, and surpasses the best 34B model (LLaMa 34B, [25]) in mathematics and code generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [20], without sacrificing performance on non-code related benchmarks. Mistral 7B leverages grouped-query attention (GQA) [1], and sliding window attention (SWA) [6, 3]. GQA significantly accelerates the inference speed, and also reduces the memory requirement during decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms collectively contribute to the enhanced performance and efficiency of Mistral 7B.',\n",
       "    'md': 'In the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model performance often necessitates an escalation in model size. However, this scaling tends to increase computational costs and inference latency, thereby raising barriers to deployment in practical, real-world scenarios. In this context, the search for balanced models delivering both high-level performance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that a carefully designed language model can deliver high performance while maintaining an efficient inference. Mistral 7B outperforms the previous best 13B model (Llama 2, [26]) across all tested benchmarks, and surpasses the best 34B model (LLaMa 34B, [25]) in mathematics and code generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [20], without sacrificing performance on non-code related benchmarks. Mistral 7B leverages grouped-query attention (GQA) [1], and sliding window attention (SWA) [6, 3]. GQA significantly accelerates the inference speed, and also reduces the memory requirement during decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms collectively contribute to the enhanced performance and efficiency of Mistral 7B.',\n",
       "    'bBox': {'x': 107, 'y': 114, 'w': 398.2470401999999, 'h': 17.2154}}]},\n",
       " {'page': 2,\n",
       "  'text': 'Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\\nimplementation1 facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\\nor Azure using the vLLM [17 ] inference server and SkyPilot 2 . Integration with Hugging Face 3 is\\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\\nmodel fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping\\nlarge language models efficient. Through our work, our aim is to help the community create more\\naffordable, efficient, and high-performing language models that can be used in a wide range of\\nreal-world applications.\\n\\n2     Architectural details\\n          The     cat    sat     on    the           The     cat   sat     on     the                               window size\\n The\\n  cat\\n  sat                                                                                       Layers\\n  on\\n  the                                                                                                         Tokens\\nFigure 1: Sliding Window Attention. The number of operations in vanilla attention is quadratic in the sequence\\nlength, and the memory increases linearly with the number of tokens. At inference time, this incurs higher\\nlatency and smaller throughput due to reduced cache availability. To alleviate this issue, we use sliding window\\nattention: each token can attend to at most W tokens from the previous layer (here, W                  = 3 ). Note that tokens\\noutside the sliding window still influence next word prediction. At each attention layer, information can move\\nforward by W tokens. Hence, after k attention layers, information can move forward by up to k ⇥ W                        tokens.\\n\\nMistral 7B is based on a transformer architecture [27 ].                      The main            Parameter            Value\\nparameters of the architecture are summarized in Table 1. Compared\\nto Llama, it introduces a few changes that we summarize below.                                    dim                   4096\\nSliding Window Attention. SWA exploits the stacked layers of a trans-                             n_layers                 32\\nformer to attend information beyond the window size W . The hidden                                head_dim                128\\n                                                                                                  hidden_dim           14336\\nstate in position i of the layer k , h , attends to all hidden states fromi                       n_heads                  32\\nthe previous layer with positions between i \\x00 W                   and i . Recursively,            n_kv_heads                 8\\nh i can access tokens from the input layer at a distance of up to W ⇥ k                           window_size           4096\\ntokens, as illustrated in Figure 1. At the last layer, using a window size                        context_len           8192\\nof W    = 4096, we have a theoretical attention span of approximately                             vocab_size           32000\\n131K tokens. In practice, for a sequence length of 16K and W                     = 4096,      Table 1: Model architecture.\\nchanges made to FlashAttention [ 11 ] and xFormers [18 ] yield a 2x\\nspeed improvement over a vanilla attention baseline.\\nRolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling\\nbuffer cache. The cache has a fixed size of W , and the keys and values for the timestep i are stored\\nin position i mod W          of the cache. As a result, when the position i is larger than W , past values\\nin the cache are overwritten, and the size of the cache stops increasing. We provide an illustration\\nin Figure 2 for W       = 3. On a sequence length of 32k tokens, this reduces the cache memory usage\\nby 8x, without impacting the model quality.\\n    1 https://github.com/mistralai/mistral-src\\n    2 https://github.com/skypilot-org/skypilot\\n    3 https://huggingface.co/mistralai\\n\\n                                                                2',\n",
       "  'md': '# Mistral 7B\\n\\nMistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference implementation facilitating easy deployment either locally or on cloud platforms such as AWS, GCP, or Azure using the vLLM inference server and SkyPilot. Integration with Hugging Face is also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat model fine-tuned from Mistral 7B that significantly outperforms the Llama 13B – Chat model. Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping large language models efficient. Through our work, our aim is to help the community create more affordable, efficient, and high-performing language models that can be used in a wide range of real-world applications.\\n\\n# Architectural details\\n\\n|Parameter|Value|\\n|---|---|\\n|dim|4096|\\n|n_layers|32|\\n|head_dim|128|\\n|hidden_dim|14336|\\n|n_heads|32|\\n|n_kv_heads|8|\\n|window_size|4096|\\n|context_len|8192|\\n|vocab_size|32000|\\n\\nSliding Window Attention. SWA exploits the stacked layers of a transformer to attend information beyond the window size W. The hidden state in position i of the layer k, h, attends to all hidden states from the previous layer with positions between i - W and i. Recursively, h_i can access tokens from the input layer at a distance of up to W x k tokens. At the last layer, using a window size of W = 4096, we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K and W = 4096, changes made to FlashAttention and xFormers yield a 2x speed improvement over a vanilla attention baseline.\\n\\nRolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality.\\n\\nReferences:\\n\\n1. Mistral 7B Source\\n2. SkyPilot\\n3. Hugging Face',\n",
       "  'images': [{'name': 'img_p1_1.png',\n",
       "    'height': 295,\n",
       "    'width': 793,\n",
       "    'x': 109.976,\n",
       "    'y': 234.609,\n",
       "    'original_width': 2148,\n",
       "    'original_height': 800,\n",
       "    'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p1_1.png',\n",
       "    'job_id': '12810641-4017-4eb7-9ac3-4129c2421dbc',\n",
       "    'original_pdf_path': './files/Mistral_7B.pdf',\n",
       "    'page_number': 2}],\n",
       "  'items': [{'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Mistral 7B',\n",
       "    'md': '# Mistral 7B',\n",
       "    'bBox': {'x': 0, 'y': 0, 'w': 612}},\n",
       "   {'type': 'text',\n",
       "    'value': 'Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference implementation facilitating easy deployment either locally or on cloud platforms such as AWS, GCP, or Azure using the vLLM inference server and SkyPilot. Integration with Hugging Face is also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat model fine-tuned from Mistral 7B that significantly outperforms the Llama 13B – Chat model. Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping large language models efficient. Through our work, our aim is to help the community create more affordable, efficient, and high-performing language models that can be used in a wide range of real-world applications.',\n",
       "    'md': 'Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference implementation facilitating easy deployment either locally or on cloud platforms such as AWS, GCP, or Azure using the vLLM inference server and SkyPilot. Integration with Hugging Face is also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat model fine-tuned from Mistral 7B that significantly outperforms the Llama 13B – Chat model. Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping large language models efficient. Through our work, our aim is to help the community create more affordable, efficient, and high-performing language models that can be used in a wide range of real-world applications.',\n",
       "    'bBox': {'x': 108, 'y': 82, 'w': 396.0023651009999, 'h': 9.9626}},\n",
       "   {'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Architectural details',\n",
       "    'md': '# Architectural details',\n",
       "    'bBox': {'x': 125, 'y': 215, 'w': 105.03838720000002, 'h': 11.9552}},\n",
       "   {'type': 'table',\n",
       "    'rows': [['Parameter', 'Value'],\n",
       "     ['dim', '4096'],\n",
       "     ['n_layers', '32'],\n",
       "     ['head_dim', '128'],\n",
       "     ['hidden_dim', '14336'],\n",
       "     ['n_heads', '32'],\n",
       "     ['n_kv_heads', '8'],\n",
       "     ['window_size', '4096'],\n",
       "     ['context_len', '8192'],\n",
       "     ['vocab_size', '32000']],\n",
       "    'md': '|Parameter|Value|\\n|---|---|\\n|dim|4096|\\n|n_layers|32|\\n|head_dim|128|\\n|hidden_dim|14336|\\n|n_heads|32|\\n|n_kv_heads|8|\\n|window_size|4096|\\n|context_len|8192|\\n|vocab_size|32000|',\n",
       "    'isPerfectTable': True,\n",
       "    'csv': '\"Parameter\",\"Value\"\\n\"dim\",\"4096\"\\n\"n_layers\",\"32\"\\n\"head_dim\",\"128\"\\n\"hidden_dim\",\"14336\"\\n\"n_heads\",\"32\"\\n\"n_kv_heads\",\"8\"\\n\"window_size\",\"4096\"\\n\"context_len\",\"8192\"\\n\"vocab_size\",\"32000\"'},\n",
       "   {'type': 'text',\n",
       "    'value': 'Sliding Window Attention. SWA exploits the stacked layers of a transformer to attend information beyond the window size W. The hidden state in position i of the layer k, h, attends to all hidden states from the previous layer with positions between i - W and i. Recursively, h_i can access tokens from the input layer at a distance of up to W x k tokens. At the last layer, using a window size of W = 4096, we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K and W = 4096, changes made to FlashAttention and xFormers yield a 2x speed improvement over a vanilla attention baseline.\\n\\nRolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality.\\n\\nReferences:\\n\\n1. Mistral 7B Source\\n2. SkyPilot\\n3. Hugging Face',\n",
       "    'md': 'Sliding Window Attention. SWA exploits the stacked layers of a transformer to attend information beyond the window size W. The hidden state in position i of the layer k, h, attends to all hidden states from the previous layer with positions between i - W and i. Recursively, h_i can access tokens from the input layer at a distance of up to W x k tokens. At the last layer, using a window size of W = 4096, we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K and W = 4096, changes made to FlashAttention and xFormers yield a 2x speed improvement over a vanilla attention baseline.\\n\\nRolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality.\\n\\nReferences:\\n\\n1. Mistral 7B Source\\n2. SkyPilot\\n3. Hugging Face',\n",
       "    'bBox': {'x': 107,\n",
       "     'y': 236.09390847457627,\n",
       "     'w': 395.05741837599976,\n",
       "     'h': 9.9626}}]},\n",
       " {'page': 3,\n",
       "  'text': '                           This                                 This                  example                              example\\n                          Mistral                             Mistral                              Mistral                  good\\n                           The      cat     sat                 the     cat     sat                  the     mat\\nFigure 2: Rolling buffer cache. The cache has a fixed size of W               = 4 . Keys and values for position i are stored\\nin position i mod W       of the cache. When the position i is larger than W , past values in the cache are overwritten.\\nThe hidden state corresponding to the latest generated tokens are colored in orange.\\n\\nPre-fill and Chunking.            When generating a sequence, we need to predict tokens one-by-one, as\\neach token is conditioned on the previous ones. However, the prompt is known in advance, and we\\ncan pre-fill the ( k , v ) cache with the prompt. If the prompt is very large, we can chunk it into smaller\\npieces, and pre-fill the cache with each chunk. For this purpose, we can select the window size as\\nour chunk size. For each chunk, we thus need to compute the attention over the cache and over the\\nchunk. Figure 3 shows how the attention mask works over both the cache and the chunk.\\n\\n                             The     cat    sat    on     the   mat    and    saw    the    dog     go     to\\n\\n                     the       0      0      0      0      0      1      1     1      1      0      0      0\\n\\n                     dog       0      0      0      0      0      0      1     1      1      1      0      0\\n\\n                      go       0      0      0      0      0      0      0     1      1      1      1      0\\n\\n                      to       0      0      0      0      0      0      0     0      1      1      1      1\\n                                        Past                        Cache                      Current\\n\\nFigure 3: Pre-fill and chunking.          During pre-fill of the cache, long sequences are chunked to limit memory\\nusage. We process a sequence in three chunks, “The cat sat on”, “the mat and saw”, “the dog go to”. The figure\\nshows what happens for the third chunk (“the dog go to”): it attends itself using a causal mask (rightmost block),\\nattends the cache using a sliding window (center block), and does not attend to past tokens as they are outside of\\nthe sliding window (left block).\\n\\n3     Results\\nWe compare Mistral 7B to Llama, and re-run all benchmarks with our own evaluation pipeline for\\nfair comparison. We measure performance on a wide variety of tasks categorized as follow:\\n\\n • Commonsense Reasoning (0-shot):                     Hellaswag [ 28 ], Winogrande [21 ], PIQA [4 ], SIQA [22 ],\\n   OpenbookQA [19], ARC-Easy, ARC-Challenge [9], CommonsenseQA [24]\\n • World Knowledge (5-shot): NaturalQuestions [16], TriviaQA [15]\\n • Reading Comprehension (0-shot): BoolQ [8], QuAC [7]\\n • Math: GSM8K [10] (8-shot) with maj@8 and MATH [13] (4-shot) with maj@4\\n • Code: Humaneval [5] (0-shot) and MBPP [2] (3-shot)\\n • Popular aggregated results:                MMLU [ 12] (5-shot), BBH [23] (3-shot), and AGI Eval [ 29 ]\\n   (3-5-shot, English multiple-choice questions only)\\n\\nDetailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4\\ncompares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4 in different\\ncategories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on\\nmost benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics,\\nand reasoning benchmarks.\\n    4 Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n\\n                                                                 3',\n",
       "  'md': '# Figure 2: Rolling buffer cache\\n\\nThe cache has a fixed size of W = 4. Keys and values for position i are stored in position i mod W of the cache. When the position i is larger than W, past values in the cache are overwritten. The hidden state corresponding to the latest generated tokens are colored in orange.\\n\\n# Pre-fill and Chunking\\n\\nWhen generating a sequence, we need to predict tokens one-by-one, as each token is conditioned on the previous ones. However, the prompt is known in advance, and we can pre-fill the (k, v) cache with the prompt. If the prompt is very large, we can chunk it into smaller pieces, and pre-fill the cache with each chunk. For this purpose, we can select the window size as our chunk size. For each chunk, we thus need to compute the attention over the cache and over the chunk. Figure 3 shows how the attention mask works over both the cache and the chunk.\\n\\n| |The|cat|sat|on|the|mat|and|saw|dog|go|to|\\n|---|---|---|---|---|---|---|---|---|---|---|---|\\n|the|0|0|0|0|0|1|1|1|0|0|0|\\n|dog|0|0|0|0|0|0|1|1|1|0|0|\\n|go|0|0|0|0|0|0|0|1|1|1|0|\\n|to|0|0|0|0|0|0|0|0|1|1|1|\\n\\n# Figure 3: Pre-fill and chunking\\n\\nDuring pre-fill of the cache, long sequences are chunked to limit memory usage. We process a sequence in three chunks, “The cat sat on”, “the mat and saw”, “the dog go to”. The figure shows what happens for the third chunk (“the dog go to”): it attends itself using a causal mask (rightmost block), attends the cache using a sliding window (center block), and does not attend to past tokens as they are outside of the sliding window (left block).\\n\\n# Results\\n\\nWe compare Mistral 7B to Llama, and re-run all benchmarks with our own evaluation pipeline for fair comparison. We measure performance on a wide variety of tasks categorized as follows:\\n\\n- Commonsense Reasoning (0-shot): Hellaswag [28], Winogrande [21], PIQA [4], SIQA [22], OpenbookQA [19], ARC-Easy, ARC-Challenge [9], CommonsenseQA [24]\\n- World Knowledge (5-shot): NaturalQuestions [16], TriviaQA [15]\\n- Reading Comprehension (0-shot): BoolQ [8], QuAC [7]\\n- Math: GSM8K [10] (8-shot) with maj@8 and MATH [13] (4-shot) with maj@4\\n- Code: Humaneval [5] (0-shot) and MBPP [2] (3-shot)\\n- Popular aggregated results: MMLU [12] (5-shot), BBH [23] (3-shot), and AGI Eval [29] (3-5-shot, English multiple-choice questions only)\\n\\nDetailed results for Mistral 7B, Llama 27B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 27B/13B, and Llama 1 34B4 in different categories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on most benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics, and reasoning benchmarks.\\n\\nSince Llama 2 34B was not open-sourced, we report results for Llama 1 34B.',\n",
       "  'images': [{'name': 'img_p2_1.png',\n",
       "    'height': 127,\n",
       "    'width': 800,\n",
       "    'x': 108.008,\n",
       "    'y': 72.00066999999996,\n",
       "    'original_width': 2552,\n",
       "    'original_height': 402,\n",
       "    'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p2_1.png',\n",
       "    'job_id': '12810641-4017-4eb7-9ac3-4129c2421dbc',\n",
       "    'original_pdf_path': './files/Mistral_7B.pdf',\n",
       "    'page_number': 3}],\n",
       "  'items': [{'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Figure 2: Rolling buffer cache',\n",
       "    'md': '# Figure 2: Rolling buffer cache',\n",
       "    'bBox': {'x': 311, 'y': 379, 'w': 18.14525761356005, 'h': 6.2773326}},\n",
       "   {'type': 'text',\n",
       "    'value': 'The cache has a fixed size of W = 4. Keys and values for position i are stored in position i mod W of the cache. When the position i is larger than W, past values in the cache are overwritten. The hidden state corresponding to the latest generated tokens are colored in orange.',\n",
       "    'md': 'The cache has a fixed size of W = 4. Keys and values for position i are stored in position i mod W of the cache. When the position i is larger than W, past values in the cache are overwritten. The hidden state corresponding to the latest generated tokens are colored in orange.',\n",
       "    'bBox': {'x': 107,\n",
       "     'y': 123.57165149606296,\n",
       "     'w': 299.4687935999998,\n",
       "     'h': 8.9664}},\n",
       "   {'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Pre-fill and Chunking',\n",
       "    'md': '# Pre-fill and Chunking',\n",
       "    'bBox': {'x': 322, 'y': 279, 'w': 13.96413561312004, 'h': 8.3697768}},\n",
       "   {'type': 'text',\n",
       "    'value': 'When generating a sequence, we need to predict tokens one-by-one, as each token is conditioned on the previous ones. However, the prompt is known in advance, and we can pre-fill the (k, v) cache with the prompt. If the prompt is very large, we can chunk it into smaller pieces, and pre-fill the cache with each chunk. For this purpose, we can select the window size as our chunk size. For each chunk, we thus need to compute the attention over the cache and over the chunk. Figure 3 shows how the attention mask works over both the cache and the chunk.',\n",
       "    'md': 'When generating a sequence, we need to predict tokens one-by-one, as each token is conditioned on the previous ones. However, the prompt is known in advance, and we can pre-fill the (k, v) cache with the prompt. If the prompt is very large, we can chunk it into smaller pieces, and pre-fill the cache with each chunk. For this purpose, we can select the window size as our chunk size. For each chunk, we thus need to compute the attention over the cache and over the chunk. Figure 3 shows how the attention mask works over both the cache and the chunk.',\n",
       "    'bBox': {'x': 108,\n",
       "     'y': 92.13791039370075,\n",
       "     'w': 395.9990616500001,\n",
       "     'h': 11.95519999999999}},\n",
       "   {'type': 'table',\n",
       "    'rows': [['',\n",
       "      'The',\n",
       "      'cat',\n",
       "      'sat',\n",
       "      'on',\n",
       "      'the',\n",
       "      'mat',\n",
       "      'and',\n",
       "      'saw',\n",
       "      'dog',\n",
       "      'go',\n",
       "      'to'],\n",
       "     ['the', '0', '0', '0', '0', '0', '1', '1', '1', '0', '0', '0'],\n",
       "     ['dog', '0', '0', '0', '0', '0', '0', '1', '1', '1', '0', '0'],\n",
       "     ['go', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '0'],\n",
       "     ['to', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1']],\n",
       "    'md': '| |The|cat|sat|on|the|mat|and|saw|dog|go|to|\\n|---|---|---|---|---|---|---|---|---|---|---|---|\\n|the|0|0|0|0|0|1|1|1|0|0|0|\\n|dog|0|0|0|0|0|0|1|1|1|0|0|\\n|go|0|0|0|0|0|0|0|1|1|1|0|\\n|to|0|0|0|0|0|0|0|0|1|1|1|',\n",
       "    'isPerfectTable': True,\n",
       "    'csv': '\"\",\"The\",\"cat\",\"sat\",\"on\",\"the\",\"mat\",\"and\",\"saw\",\"dog\",\"go\",\"to\"\\n\"the\",\"0\",\"0\",\"0\",\"0\",\"0\",\"1\",\"1\",\"1\",\"0\",\"0\",\"0\"\\n\"dog\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"1\",\"1\",\"1\",\"0\",\"0\"\\n\"go\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"1\",\"1\",\"1\",\"0\"\\n\"to\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"1\",\"1\",\"1\"'},\n",
       "   {'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Figure 3: Pre-fill and chunking',\n",
       "    'md': '# Figure 3: Pre-fill and chunking',\n",
       "    'bBox': {'x': 303, 'y': 279, 'w': 32.96413561312004, 'h': 9.9626}},\n",
       "   {'type': 'text',\n",
       "    'value': 'During pre-fill of the cache, long sequences are chunked to limit memory usage. We process a sequence in three chunks, “The cat sat on”, “the mat and saw”, “the dog go to”. The figure shows what happens for the third chunk (“the dog go to”): it attends itself using a causal mask (rightmost block), attends the cache using a sliding window (center block), and does not attend to past tokens as they are outside of the sliding window (left block).',\n",
       "    'md': 'During pre-fill of the cache, long sequences are chunked to limit memory usage. We process a sequence in three chunks, “The cat sat on”, “the mat and saw”, “the dog go to”. The figure shows what happens for the third chunk (“the dog go to”): it attends itself using a causal mask (rightmost block), attends the cache using a sliding window (center block), and does not attend to past tokens as they are outside of the sliding window (left block).',\n",
       "    'bBox': {'x': 108,\n",
       "     'y': 123.57165149606296,\n",
       "     'w': 397.122931968,\n",
       "     'h': 8.9664}},\n",
       "   {'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Results',\n",
       "    'md': '# Results',\n",
       "    'bBox': {'x': 125, 'y': 481, 'w': 37.1926272, 'h': 11.9552}},\n",
       "   {'type': 'text',\n",
       "    'value': 'We compare Mistral 7B to Llama, and re-run all benchmarks with our own evaluation pipeline for fair comparison. We measure performance on a wide variety of tasks categorized as follows:\\n\\n- Commonsense Reasoning (0-shot): Hellaswag [28], Winogrande [21], PIQA [4], SIQA [22], OpenbookQA [19], ARC-Easy, ARC-Challenge [9], CommonsenseQA [24]\\n- World Knowledge (5-shot): NaturalQuestions [16], TriviaQA [15]\\n- Reading Comprehension (0-shot): BoolQ [8], QuAC [7]\\n- Math: GSM8K [10] (8-shot) with maj@8 and MATH [13] (4-shot) with maj@4\\n- Code: Humaneval [5] (0-shot) and MBPP [2] (3-shot)\\n- Popular aggregated results: MMLU [12] (5-shot), BBH [23] (3-shot), and AGI Eval [29] (3-5-shot, English multiple-choice questions only)\\n\\nDetailed results for Mistral 7B, Llama 27B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 27B/13B, and Llama 1 34B4 in different categories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on most benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics, and reasoning benchmarks.\\n\\nSince Llama 2 34B was not open-sourced, we report results for Llama 1 34B.',\n",
       "    'md': 'We compare Mistral 7B to Llama, and re-run all benchmarks with our own evaluation pipeline for fair comparison. We measure performance on a wide variety of tasks categorized as follows:\\n\\n- Commonsense Reasoning (0-shot): Hellaswag [28], Winogrande [21], PIQA [4], SIQA [22], OpenbookQA [19], ARC-Easy, ARC-Challenge [9], CommonsenseQA [24]\\n- World Knowledge (5-shot): NaturalQuestions [16], TriviaQA [15]\\n- Reading Comprehension (0-shot): BoolQ [8], QuAC [7]\\n- Math: GSM8K [10] (8-shot) with maj@8 and MATH [13] (4-shot) with maj@4\\n- Code: Humaneval [5] (0-shot) and MBPP [2] (3-shot)\\n- Popular aggregated results: MMLU [12] (5-shot), BBH [23] (3-shot), and AGI Eval [29] (3-5-shot, English multiple-choice questions only)\\n\\nDetailed results for Mistral 7B, Llama 27B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 27B/13B, and Llama 1 34B4 in different categories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on most benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics, and reasoning benchmarks.\\n\\nSince Llama 2 34B was not open-sourced, we report results for Llama 1 34B.',\n",
       "    'bBox': {'x': 107,\n",
       "     'y': 107.85478094488185,\n",
       "     'w': 397.2480119999998,\n",
       "     'h': 11.9552}}]},\n",
       " {'page': 4,\n",
       "  'text': '                          Mista| 7R      ULJMA 138                                           Mistral 78     LLaMA348\\n                                                                                             Mava           ILAMA\\n                                                                     1\\n            MMLU        Knowledge     Reasoning   Comprehension                AGI Eval       Math          BBH          Code\\nFigure 4: Performance of Mistral 7B and different Llama models on a wide range of benchmarks. All\\nmodels were re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mistral 7B\\nsignificantly outperforms Llama 2 7B and Llama 2 13B on all benchmarks. It is also vastly superior to Llama 1\\n34B in mathematics, code generation, and reasoning benchmarks.\\n\\nModel            Modality  MMLU HellaSwag WinoG        PIQA    Arc-e  Arc-c    NQ    TriviaQA HumanEval MBPP MATH GSM8K\\nLLaMA 2 7B      Pretrained  44.4%     77.1%    69.5%   77.9%   68.7%  43.2%   24.7%   63.8%      11.6%     26.1%    3.9%   16.0%\\nLLaMA 2 13B     Pretrained  55.6%    80.7%     72.9%   80.8%   75.2%  48.8%   29.0%   69.6%      18.9%     35.4%    6.0%   34.3%\\nCode-Llama 7B Finetuned     36.9%     62.9%    62.3%   72.8%   59.4%  34.5%   11.0%   34.9%      31.1%     52.5%    5.2%   20.8%\\nMistral 7B      Pretrained 60.1%     81.3%     75.3% 83.0% 80.0% 55.5% 28.8%          69.9%      30.5%     47.5%   13.1%   52.2%\\n\\nTable 2: Comparison of Mistral 7B with Llama. Mistral 7B outperforms Llama 2 13B on all metrics, and\\napproaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchmarks.\\n\\nSize and Efficiency.           We computed “equivalent model sizes” of the Llama 2 family, aiming to\\nunderstand Mistral 7B models’ efficiency in the cost-performance spectrum (see Figure 5). When\\nevaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B\\nmirrored performance that one might expect from a Llama 2 model with more than 3x its size. On\\nthe Knowledge benchmarks, Mistral 7B’s performance achieves a lower compression rate of 1.9x,\\nwhich is likely due to its limited parameter count that restricts the amount of knowledge it can store.\\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation\\nprotocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2)\\non TriviaQA, we do not provide Wikipedia contexts.\\n\\n4     Instruction Finetuning\\nTo evaluate the generalization capabilities of                                                 Chatbot Arena\\nMistral 7B, we fine-tuned it on instruction datasets                 Model                       ELO Rating        MT Bench\\npublicly available on the Hugging Face repository.                   WizardLM 13B v1.2               1047          7.2\\nNo proprietary data or training tricks were utilized:                Mistral 7B Instruct             1031          6.84 +/- 0.07\\nMistral 7B – Instruct model is a simple and                          Llama 2 13B Chat                1012          6.65\\npreliminary demonstration that the base model can                    Vicuna 13B                      1041          6.57\\neasily be fine-tuned to achieve good performance.                    Llama 2 7B Chat                  985          6.27\\nIn Table 3, we observe that the resulting model,                     Vicuna 7B                        997          6.17\\nMistral 7B – Instruct, exhibits superior perfor-                     Alpaca 13B                       914          4.53\\nmance compared to all 7B models on MT-Bench,                         Table 3: Comparison of Chat models. Mistral 7B –\\nand is comparable to 13B – Chat models.                      An      Instruct outperforms all 7B models on MT-Bench, and\\nindependent human evaluation was conducted on                        is comparable to 13B – Chat models.\\nhttps://llmboxing.com/leaderboard.\\nIn this evaluation, participants were provided with a set of questions along with anonymous responses\\nfrom two models and were asked to select their preferred response, as illustrated in Figure 6. As of\\nOctober 6, 2023, the outputs generated by Mistral 7B were preferred 5020 times, compared to 4143\\ntimes for Llama 2 13B.\\n\\n                                                                 4',\n",
       "  'md': '# Figure 4: Performance of Mistral 7B and different Llama models on a wide range of benchmarks.\\n\\nAll models were re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mistral 7B significantly outperforms Llama 2 7B and Llama 2 13B on all benchmarks. It is also vastly superior to Llama 134B in mathematics, code generation, and reasoning benchmarks.\\n\\n|Model|Modality|MMLU|HellaSwag|WinoG|PIQA|Arc-e|Arc-c|NQ|TriviaQA|HumanEval|MBPP|MATH|GSM8K|\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|LLaMA 2 7B|Pretrained|44.4%|77.1%|69.5%|77.9%|68.7%|43.2%|24.7%|63.8%|11.6%|26.1%|3.9%|16.0%|\\n|LLaMA 2 13B|Pretrained|55.6%|80.7%|72.9%|80.8%|75.2%|48.8%|29.0%|69.6%|18.9%|35.4%|6.0%|34.3%|\\n|Code-Llama 7B|Finetuned|36.9%|62.9%|62.3%|72.8%|59.4%|34.5%|11.0%|34.9%|31.1%|52.5%|5.2%|20.8%|\\n|Mistral 7B|Pretrained|60.1%|81.3%|75.3%|83.0%|80.0%|55.5%|28.8%|69.9%|30.5%|47.5%|13.1%|52.2%|\\n\\n# Table 2: Comparison of Mistral 7B with Llama.\\n\\nMistral 7B outperforms Llama 2 13B on all metrics, and approaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchmarks.\\n\\n# Size and Efficiency.\\n\\nWe computed “equivalent model sizes” of the Llama 2 family, aiming to understand Mistral 7B models’ efficiency in the cost-performance spectrum (see Figure 5). When evaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B mirrored performance that one might expect from a Llama 2 model with more than 3x its size. On the Knowledge benchmarks, Mistral 7B’s performance achieves a lower compression rate of 1.9x, which is likely due to its limited parameter count that restricts the amount of knowledge it can store.\\n\\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\\n\\n# Instruction Finetuning\\n\\nTo evaluate the generalization capabilities of Mistral 7B, we fine-tuned it on instruction datasets publicly available on the Hugging Face repository. No proprietary data or training tricks were utilized: Mistral 7B – Instruct model is a simple and preliminary demonstration that the base model can easily be fine-tuned to achieve good performance. In Table 3, we observe that the resulting model, Mistral 7B – Instruct, exhibits superior performance compared to all 7B models on MT-Bench, and is comparable to 13B – Chat models.\\n\\n|Model|ELO Rating|MT Bench|\\n|---|---|---|\\n|WizardLM 13B v1.2|1047|7.2|\\n|Mistral 7B Instruct|1031|6.84 +/- 0.07|\\n|Llama 2 13B Chat|1012|6.65|\\n|Vicuna 13B|1041|6.57|\\n|Llama 2 7B Chat|985|6.27|\\n|Vicuna 7B|997|6.17|\\n|Alpaca 13B|914|4.53|\\n\\nTable 3: Comparison of Chat models. Mistral 7B – Instruct outperforms all 7B models on MT-Bench, and is comparable to 13B – Chat models.\\n\\nAn independent human evaluation was conducted on llmboxing.com/leaderboard.\\n\\nIn this evaluation, participants were provided with a set of questions along with anonymous responses from two models and were asked to select their preferred response, as illustrated in Figure 6. As of October 6, 2023, the outputs generated by Mistral 7B were preferred 5020 times, compared to 4143 times for Llama 2 13B.',\n",
       "  'images': [{'name': 'img_p3_1.png',\n",
       "    'height': 258,\n",
       "    'width': 792,\n",
       "    'x': 110.018,\n",
       "    'y': 72.00260000000003,\n",
       "    'original_width': 10973,\n",
       "    'original_height': 3570,\n",
       "    'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p3_1.png',\n",
       "    'job_id': '12810641-4017-4eb7-9ac3-4129c2421dbc',\n",
       "    'original_pdf_path': './files/Mistral_7B.pdf',\n",
       "    'page_number': 4}],\n",
       "  'items': [{'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Figure 4: Performance of Mistral 7B and different Llama models on a wide range of benchmarks.',\n",
       "    'md': '# Figure 4: Performance of Mistral 7B and different Llama models on a wide range of benchmarks.',\n",
       "    'bBox': {'x': 108, 'y': 320, 'w': 32.623999999999995, 'h': 9.9626}},\n",
       "   {'type': 'text',\n",
       "    'value': 'All models were re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mistral 7B significantly outperforms Llama 2 7B and Llama 2 13B on all benchmarks. It is also vastly superior to Llama 134B in mathematics, code generation, and reasoning benchmarks.',\n",
       "    'md': 'All models were re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mistral 7B significantly outperforms Llama 2 7B and Llama 2 13B on all benchmarks. It is also vastly superior to Llama 134B in mathematics, code generation, and reasoning benchmarks.',\n",
       "    'bBox': {'x': 108,\n",
       "     'y': 127.36029302325584,\n",
       "     'w': 396.67317721399957,\n",
       "     'h': 26.690316279069762}},\n",
       "   {'type': 'table',\n",
       "    'rows': [['Model',\n",
       "      'Modality',\n",
       "      'MMLU',\n",
       "      'HellaSwag',\n",
       "      'WinoG',\n",
       "      'PIQA',\n",
       "      'Arc-e',\n",
       "      'Arc-c',\n",
       "      'NQ',\n",
       "      'TriviaQA',\n",
       "      'HumanEval',\n",
       "      'MBPP',\n",
       "      'MATH',\n",
       "      'GSM8K'],\n",
       "     ['LLaMA 2 7B',\n",
       "      'Pretrained',\n",
       "      '44.4%',\n",
       "      '77.1%',\n",
       "      '69.5%',\n",
       "      '77.9%',\n",
       "      '68.7%',\n",
       "      '43.2%',\n",
       "      '24.7%',\n",
       "      '63.8%',\n",
       "      '11.6%',\n",
       "      '26.1%',\n",
       "      '3.9%',\n",
       "      '16.0%'],\n",
       "     ['LLaMA 2 13B',\n",
       "      'Pretrained',\n",
       "      '55.6%',\n",
       "      '80.7%',\n",
       "      '72.9%',\n",
       "      '80.8%',\n",
       "      '75.2%',\n",
       "      '48.8%',\n",
       "      '29.0%',\n",
       "      '69.6%',\n",
       "      '18.9%',\n",
       "      '35.4%',\n",
       "      '6.0%',\n",
       "      '34.3%'],\n",
       "     ['Code-Llama 7B',\n",
       "      'Finetuned',\n",
       "      '36.9%',\n",
       "      '62.9%',\n",
       "      '62.3%',\n",
       "      '72.8%',\n",
       "      '59.4%',\n",
       "      '34.5%',\n",
       "      '11.0%',\n",
       "      '34.9%',\n",
       "      '31.1%',\n",
       "      '52.5%',\n",
       "      '5.2%',\n",
       "      '20.8%'],\n",
       "     ['Mistral 7B',\n",
       "      'Pretrained',\n",
       "      '60.1%',\n",
       "      '81.3%',\n",
       "      '75.3%',\n",
       "      '83.0%',\n",
       "      '80.0%',\n",
       "      '55.5%',\n",
       "      '28.8%',\n",
       "      '69.9%',\n",
       "      '30.5%',\n",
       "      '47.5%',\n",
       "      '13.1%',\n",
       "      '52.2%']],\n",
       "    'md': '|Model|Modality|MMLU|HellaSwag|WinoG|PIQA|Arc-e|Arc-c|NQ|TriviaQA|HumanEval|MBPP|MATH|GSM8K|\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|LLaMA 2 7B|Pretrained|44.4%|77.1%|69.5%|77.9%|68.7%|43.2%|24.7%|63.8%|11.6%|26.1%|3.9%|16.0%|\\n|LLaMA 2 13B|Pretrained|55.6%|80.7%|72.9%|80.8%|75.2%|48.8%|29.0%|69.6%|18.9%|35.4%|6.0%|34.3%|\\n|Code-Llama 7B|Finetuned|36.9%|62.9%|62.3%|72.8%|59.4%|34.5%|11.0%|34.9%|31.1%|52.5%|5.2%|20.8%|\\n|Mistral 7B|Pretrained|60.1%|81.3%|75.3%|83.0%|80.0%|55.5%|28.8%|69.9%|30.5%|47.5%|13.1%|52.2%|',\n",
       "    'isPerfectTable': True,\n",
       "    'csv': '\"Model\",\"Modality\",\"MMLU\",\"HellaSwag\",\"WinoG\",\"PIQA\",\"Arc-e\",\"Arc-c\",\"NQ\",\"TriviaQA\",\"HumanEval\",\"MBPP\",\"MATH\",\"GSM8K\"\\n\"LLaMA 2 7B\",\"Pretrained\",\"44.4%\",\"77.1%\",\"69.5%\",\"77.9%\",\"68.7%\",\"43.2%\",\"24.7%\",\"63.8%\",\"11.6%\",\"26.1%\",\"3.9%\",\"16.0%\"\\n\"LLaMA 2 13B\",\"Pretrained\",\"55.6%\",\"80.7%\",\"72.9%\",\"80.8%\",\"75.2%\",\"48.8%\",\"29.0%\",\"69.6%\",\"18.9%\",\"35.4%\",\"6.0%\",\"34.3%\"\\n\"Code-Llama 7B\",\"Finetuned\",\"36.9%\",\"62.9%\",\"62.3%\",\"72.8%\",\"59.4%\",\"34.5%\",\"11.0%\",\"34.9%\",\"31.1%\",\"52.5%\",\"5.2%\",\"20.8%\"\\n\"Mistral 7B\",\"Pretrained\",\"60.1%\",\"81.3%\",\"75.3%\",\"83.0%\",\"80.0%\",\"55.5%\",\"28.8%\",\"69.9%\",\"30.5%\",\"47.5%\",\"13.1%\",\"52.2%\"'},\n",
       "   {'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Table 2: Comparison of Mistral 7B with Llama.',\n",
       "    'md': '# Table 2: Comparison of Mistral 7B with Llama.',\n",
       "    'bBox': {'x': 108,\n",
       "     'y': 320,\n",
       "     'w': 32.623999999999995,\n",
       "     'h': 6.973799999999983}},\n",
       "   {'type': 'text',\n",
       "    'value': 'Mistral 7B outperforms Llama 2 13B on all metrics, and approaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchmarks.',\n",
       "    'md': 'Mistral 7B outperforms Llama 2 13B on all metrics, and approaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchmarks.',\n",
       "    'bBox': {'x': 108,\n",
       "     'y': 127.36029302325584,\n",
       "     'w': 395.8941586000001,\n",
       "     'h': 26.690316279069762}},\n",
       "   {'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Size and Efficiency.',\n",
       "    'md': '# Size and Efficiency.',\n",
       "    'bBox': {'x': 291, 'y': 650, 'w': 12.4177807, 'h': 9.9626}},\n",
       "   {'type': 'text',\n",
       "    'value': 'We computed “equivalent model sizes” of the Llama 2 family, aiming to understand Mistral 7B models’ efficiency in the cost-performance spectrum (see Figure 5). When evaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B mirrored performance that one might expect from a Llama 2 model with more than 3x its size. On the Knowledge benchmarks, Mistral 7B’s performance achieves a lower compression rate of 1.9x, which is likely due to its limited parameter count that restricts the amount of knowledge it can store.\\n\\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.',\n",
       "    'md': 'We computed “equivalent model sizes” of the Llama 2 family, aiming to understand Mistral 7B models’ efficiency in the cost-performance spectrum (see Figure 5). When evaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B mirrored performance that one might expect from a Llama 2 model with more than 3x its size. On the Knowledge benchmarks, Mistral 7B’s performance achieves a lower compression rate of 1.9x, which is likely due to its limited parameter count that restricts the amount of knowledge it can store.\\n\\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.',\n",
       "    'bBox': {'x': 107,\n",
       "     'y': 127.36029302325584,\n",
       "     'w': 398.24704019999984,\n",
       "     'h': 26.690316279069762}},\n",
       "   {'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Instruction Finetuning',\n",
       "    'md': '# Instruction Finetuning',\n",
       "    'bBox': {'x': 125, 'y': 515, 'w': 115.91761920000002, 'h': 11.9552}},\n",
       "   {'type': 'text',\n",
       "    'value': 'To evaluate the generalization capabilities of Mistral 7B, we fine-tuned it on instruction datasets publicly available on the Hugging Face repository. No proprietary data or training tricks were utilized: Mistral 7B – Instruct model is a simple and preliminary demonstration that the base model can easily be fine-tuned to achieve good performance. In Table 3, we observe that the resulting model, Mistral 7B – Instruct, exhibits superior performance compared to all 7B models on MT-Bench, and is comparable to 13B – Chat models.',\n",
       "    'md': 'To evaluate the generalization capabilities of Mistral 7B, we fine-tuned it on instruction datasets publicly available on the Hugging Face repository. No proprietary data or training tricks were utilized: Mistral 7B – Instruct model is a simple and preliminary demonstration that the base model can easily be fine-tuned to achieve good performance. In Table 3, we observe that the resulting model, Mistral 7B – Instruct, exhibits superior performance compared to all 7B models on MT-Bench, and is comparable to 13B – Chat models.',\n",
       "    'bBox': {'x': 108,\n",
       "     'y': 127.36029302325584,\n",
       "     'w': 211.35792272727278,\n",
       "     'h': 26.690316279069762}},\n",
       "   {'type': 'table',\n",
       "    'rows': [['Model', 'ELO Rating', 'MT Bench'],\n",
       "     ['WizardLM 13B v1.2', '1047', '7.2'],\n",
       "     ['Mistral 7B Instruct', '1031', '6.84 +/- 0.07'],\n",
       "     ['Llama 2 13B Chat', '1012', '6.65'],\n",
       "     ['Vicuna 13B', '1041', '6.57'],\n",
       "     ['Llama 2 7B Chat', '985', '6.27'],\n",
       "     ['Vicuna 7B', '997', '6.17'],\n",
       "     ['Alpaca 13B', '914', '4.53']],\n",
       "    'md': '|Model|ELO Rating|MT Bench|\\n|---|---|---|\\n|WizardLM 13B v1.2|1047|7.2|\\n|Mistral 7B Instruct|1031|6.84 +/- 0.07|\\n|Llama 2 13B Chat|1012|6.65|\\n|Vicuna 13B|1041|6.57|\\n|Llama 2 7B Chat|985|6.27|\\n|Vicuna 7B|997|6.17|\\n|Alpaca 13B|914|4.53|',\n",
       "    'isPerfectTable': True,\n",
       "    'csv': '\"Model\",\"ELO Rating\",\"MT Bench\"\\n\"WizardLM 13B v1.2\",\"1047\",\"7.2\"\\n\"Mistral 7B Instruct\",\"1031\",\"6.84 +/- 0.07\"\\n\"Llama 2 13B Chat\",\"1012\",\"6.65\"\\n\"Vicuna 13B\",\"1041\",\"6.57\"\\n\"Llama 2 7B Chat\",\"985\",\"6.27\"\\n\"Vicuna 7B\",\"997\",\"6.17\"\\n\"Alpaca 13B\",\"914\",\"4.53\"'},\n",
       "   {'type': 'text',\n",
       "    'value': 'Table 3: Comparison of Chat models. Mistral 7B – Instruct outperforms all 7B models on MT-Bench, and is comparable to 13B – Chat models.\\n\\nAn independent human evaluation was conducted on llmboxing.com/leaderboard.\\n\\nIn this evaluation, participants were provided with a set of questions along with anonymous responses from two models and were asked to select their preferred response, as illustrated in Figure 6. As of October 6, 2023, the outputs generated by Mistral 7B were preferred 5020 times, compared to 4143 times for Llama 2 13B.',\n",
       "    'md': 'Table 3: Comparison of Chat models. Mistral 7B – Instruct outperforms all 7B models on MT-Bench, and is comparable to 13B – Chat models.\\n\\nAn independent human evaluation was conducted on llmboxing.com/leaderboard.\\n\\nIn this evaluation, participants were provided with a set of questions along with anonymous responses from two models and were asked to select their preferred response, as illustrated in Figure 6. As of October 6, 2023, the outputs generated by Mistral 7B were preferred 5020 times, compared to 4143 times for Llama 2 13B.',\n",
       "    'bBox': {'x': 108,\n",
       "     'y': 127.36029302325584,\n",
       "     'w': 396.0013948800002,\n",
       "     'h': 26.690316279069762}}]},\n",
       " {'page': 5,\n",
       "  'text': '                                  LLaMA                                           LLaMA\\n                                  Mistral                                         Mistral\\n                     2 60                                             2\\n                     1                                                [\\n                                                 Effective LLaMA                          Effective LLaMA\\n                                                 size 23B (33X1                               38B (5 Axl\\n                                Model size (billion parameters)                  Model size (billion parameters)\\n                                  LLaMA                                           LLaMA\\n                                  Mistral                             8           Mistral\\n                                                                      |\\n                                       Effective LLaMA                                         Effective LLaMA\\n                                       Isize 13B                                               Siz0218 (3x)\\n                                Model .size (billion parameters}                 Model size (billion parameters)\\nFigure 5: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension for\\nMistral 7B and Llama 2 (7B/13B/70B). Mistral 7B largely outperforms Llama 2 13B on all evaluations, except\\non knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which limits the\\namount of knowledge it can compress).\\n\\n5     Adding guardrails for front-facing applications\\nThe ability to enforce guardrails when it comes to AI generation is important for front-facing appli-\\ncations. In this section, we highlight how to leverage system prompting to optionally enforce output\\nconstraints on top of our models. Additionally, we showcase the ability of Mistral 7B to perform\\nfine-grained content moderation, which can be useful to enforce quality content in applications.\\n\\n5.1     System prompt to enforce guardrails\\n\\nWe introduce a system prompt (see below) to guide the model to generate answers within specified\\nguardrails, similar to the work done with Llama 2. Using this prompt allows the user to move on the\\nPareto front of model utility / guardrails enforcement, as indicated in Table 4.\\n Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful,\\n unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\\nWe use a set of 175 unsafe prompts for evaluating safety.                                   Guardrails                MT Bench\\nWith the recommended system prompt, the model properly\\ndeclines to answer 100% of the harmful questions.                                       No system prompt              6.84 ± 0.07\\nAs an illustration, we provide in Table 5 the answers of                             Llama 2 system prompt            6.38 ± 0.07\\nboth Mistral 7B – Instruct and Llama 2 Chat 13B to the                               Mistral system prompt            6.58 ± 0.05\\nquestion How to kill a linux process with system prompts                           Table 4: System prompts. Mean official\\nactivated. We observe that Mistral 7B provides a correct                           MT Bench score over 10 iterations with\\nresponse while Llama 2 declines to answer. Note that on                            standard deviation for Mistral 7B – Instruct.\\nthis specific question, both models answer correctly when                          For reference, Llama 2 13B – Chat reports\\nsystem prompts are deactivated.                                                    official results of 6.65.\\n\\n5.2     Content moderation with self-reflection\\n\\nMistral 7B – Instruct can be used as a content moderator: the model itself is able to accurately\\nclassify a user prompt or its generated answer as being either acceptable or falling into one of the\\nfollowing categories: Illegal activities such as terrorism, child abuse or fraud; Hateful, harassing\\nor violent content such as discrimination, self-harm or bullying; Unqualified advice for instance\\nin legal, medical or financial domains.\\n\\n                                                                  5',\n",
       "  'md': '# LLaMA\\n\\n|Mistral|2|\\n|---|---|\\n|1|[|\\n\\n# Effective LLaMA\\n\\nsize 23B (33X1)\\n38B (5 Axl)\\n\\n# Model size (billion parameters)\\n\\n|LLaMA|Mistral|\\n|---|---|\\n| |8|\\n\\n# Effective LLaMA\\n\\nsize 13B\\nSiz0218 (3x)\\n\\n# Model size (billion parameters)\\n\\nLLaMA\\nMistral\\n\\nFigure 5: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension for Mistral 7B and Llama 2 (7B/13B/70B). Mistral 7B largely outperforms Llama 2 13B on all evaluations, except on knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which limits the amount of knowledge it can compress).\\n\\n# Adding guardrails for front-facing applications\\n\\nThe ability to enforce guardrails when it comes to AI generation is important for front-facing applications. In this section, we highlight how to leverage system prompting to optionally enforce output constraints on top of our models. Additionally, we showcase the ability of Mistral 7B to perform fine-grained content moderation, which can be useful to enforce quality content in applications.\\n\\n# System prompt to enforce guardrails\\n\\nWe introduce a system prompt (see below) to guide the model to generate answers within specified guardrails, similar to the work done with Llama 2. Using this prompt allows the user to move on the Pareto front of model utility / guardrails enforcement, as indicated in Table 4.\\n\\nAlways assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\\n\\nWe use a set of 175 unsafe prompts for evaluating safety.\\n\\n|Guardrails|MT Bench|\\n|---|---|\\n|No system prompt|6.84 ± 0.07|\\n|Llama 2 system prompt|6.38 ± 0.07|\\n|Mistral system prompt|6.58 ± 0.05|\\n\\nTable 4: System prompts. Mean official MT Bench score over 10 iterations with standard deviation for Mistral 7B – Instruct. For reference, Llama 2 13B – Chat reports official results of 6.65.\\n\\n# Content moderation with self-reflection\\n\\nMistral 7B – Instruct can be used as a content moderator: the model itself is able to accurately classify a user prompt or its generated answer as being either acceptable or falling into one of the following categories: Illegal activities such as terrorism, child abuse or fraud; Hateful, harassing or violent content such as discrimination, self-harm or bullying; Unqualified advice for instance in legal, medical or financial domains.',\n",
       "  'images': [{'name': 'img_p4_1.png',\n",
       "    'height': 388,\n",
       "    'width': 560,\n",
       "    'x': 167.432,\n",
       "    'y': 71.98650000000004,\n",
       "    'original_width': 6335,\n",
       "    'original_height': 4386,\n",
       "    'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p4_1.png',\n",
       "    'job_id': '12810641-4017-4eb7-9ac3-4129c2421dbc',\n",
       "    'original_pdf_path': './files/Mistral_7B.pdf',\n",
       "    'page_number': 5}],\n",
       "  'items': [{'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'LLaMA',\n",
       "    'md': '# LLaMA',\n",
       "    'bBox': {'x': 207.52066464285713,\n",
       "     'y': 84.35038530927838,\n",
       "     'w': 20.786715,\n",
       "     'h': 6.923775773195876}},\n",
       "   {'type': 'table',\n",
       "    'rows': [['Mistral', '2'], ['1', '[']],\n",
       "    'md': '|Mistral|2|\\n|---|---|\\n|1|[|',\n",
       "    'isPerfectTable': True,\n",
       "    'csv': '\"Mistral\",\"2\"\\n\"1\",\"[\"'},\n",
       "   {'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Effective LLaMA',\n",
       "    'md': '# Effective LLaMA',\n",
       "    'bBox': {'x': 207.52066464285713,\n",
       "     'y': 84.35038530927838,\n",
       "     'w': 42.563273571428574,\n",
       "     'h': 6.923775773195876}},\n",
       "   {'type': 'text',\n",
       "    'value': 'size 23B (33X1)\\n38B (5 Axl)',\n",
       "    'md': 'size 23B (33X1)\\n38B (5 Axl)',\n",
       "    'bBox': {'x': 108,\n",
       "     'y': 98.19793685567014,\n",
       "     'w': 212.3628317857143,\n",
       "     'h': 17.803994845360826}},\n",
       "   {'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Model size (billion parameters)',\n",
       "    'md': '# Model size (billion parameters)',\n",
       "    'bBox': {'x': 202.57144678571427,\n",
       "     'y': 165.9520283505155,\n",
       "     'w': 89.08592142857144,\n",
       "     'h': 8.407442010309278}},\n",
       "   {'type': 'table',\n",
       "    'rows': [['LLaMA', 'Mistral'], ['', '8']],\n",
       "    'md': '|LLaMA|Mistral|\\n|---|---|\\n| |8|',\n",
       "    'isPerfectTable': True,\n",
       "    'csv': '\"LLaMA\",\"Mistral\"\\n\"\",\"8\"'},\n",
       "   {'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Effective LLaMA',\n",
       "    'md': '# Effective LLaMA',\n",
       "    'bBox': {'x': 207.52066464285713,\n",
       "     'y': 84.35038530927838,\n",
       "     'w': 42.563273571428574,\n",
       "     'h': 6.923775773195876}},\n",
       "   {'type': 'text',\n",
       "    'value': 'size 13B\\nSiz0218 (3x)',\n",
       "    'md': 'size 13B\\nSiz0218 (3x)',\n",
       "    'bBox': {'x': 170.89645249999998,\n",
       "     'y': 98.19793685567014,\n",
       "     'w': 149.4663792857143,\n",
       "     'h': 17.803994845360826}},\n",
       "   {'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Model size (billion parameters)',\n",
       "    'md': '# Model size (billion parameters)',\n",
       "    'bBox': {'x': 202.57144678571427,\n",
       "     'y': 165.9520283505155,\n",
       "     'w': 89.08592142857144,\n",
       "     'h': 8.407442010309278}},\n",
       "   {'type': 'text',\n",
       "    'value': 'LLaMA\\nMistral\\n\\nFigure 5: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension for Mistral 7B and Llama 2 (7B/13B/70B). Mistral 7B largely outperforms Llama 2 13B on all evaluations, except on knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which limits the amount of knowledge it can compress).',\n",
       "    'md': 'LLaMA\\nMistral\\n\\nFigure 5: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension for Mistral 7B and Llama 2 (7B/13B/70B). Mistral 7B largely outperforms Llama 2 13B on all evaluations, except on knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which limits the amount of knowledge it can compress).',\n",
       "    'bBox': {'x': 108,\n",
       "     'y': 84.35038530927838,\n",
       "     'w': 396.2023248920001,\n",
       "     'h': 17.803994845360826}},\n",
       "   {'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Adding guardrails for front-facing applications',\n",
       "    'md': '# Adding guardrails for front-facing applications',\n",
       "    'bBox': {'x': 125, 'y': 345, 'w': 240.5984, 'h': 11.9552}},\n",
       "   {'type': 'text',\n",
       "    'value': 'The ability to enforce guardrails when it comes to AI generation is important for front-facing applications. In this section, we highlight how to leverage system prompting to optionally enforce output constraints on top of our models. Additionally, we showcase the ability of Mistral 7B to perform fine-grained content moderation, which can be useful to enforce quality content in applications.',\n",
       "    'md': 'The ability to enforce guardrails when it comes to AI generation is important for front-facing applications. In this section, we highlight how to leverage system prompting to optionally enforce output constraints on top of our models. Additionally, we showcase the ability of Mistral 7B to perform fine-grained content moderation, which can be useful to enforce quality content in applications.',\n",
       "    'bBox': {'x': 108,\n",
       "     'y': 92.26327190721653,\n",
       "     'w': 395.9971326499996,\n",
       "     'h': 9.9626}},\n",
       "   {'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'System prompt to enforce guardrails',\n",
       "    'md': '# System prompt to enforce guardrails',\n",
       "    'bBox': {'x': 130, 'y': 431, 'w': 156.00435339999999, 'h': 9.9626}},\n",
       "   {'type': 'text',\n",
       "    'value': 'We introduce a system prompt (see below) to guide the model to generate answers within specified guardrails, similar to the work done with Llama 2. Using this prompt allows the user to move on the Pareto front of model utility / guardrails enforcement, as indicated in Table 4.\\n\\nAlways assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\\n\\nWe use a set of 175 unsafe prompts for evaluating safety.',\n",
       "    'md': 'We introduce a system prompt (see below) to guide the model to generate answers within specified guardrails, similar to the work done with Llama 2. Using this prompt allows the user to move on the Pareto front of model utility / guardrails enforcement, as indicated in Table 4.\\n\\nAlways assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\\n\\nWe use a set of 175 unsafe prompts for evaluating safety.',\n",
       "    'bBox': {'x': 107,\n",
       "     'y': 84.35038530927838,\n",
       "     'w': 396.4700039999998,\n",
       "     'h': 17.803994845360826}},\n",
       "   {'type': 'table',\n",
       "    'rows': [['Guardrails', 'MT Bench'],\n",
       "     ['No system prompt', '6.84 ± 0.07'],\n",
       "     ['Llama 2 system prompt', '6.38 ± 0.07'],\n",
       "     ['Mistral system prompt', '6.58 ± 0.05']],\n",
       "    'md': '|Guardrails|MT Bench|\\n|---|---|\\n|No system prompt|6.84 ± 0.07|\\n|Llama 2 system prompt|6.38 ± 0.07|\\n|Mistral system prompt|6.58 ± 0.05|',\n",
       "    'isPerfectTable': True,\n",
       "    'csv': '\"Guardrails\",\"MT Bench\"\\n\"No system prompt\",\"6.84 ± 0.07\"\\n\"Llama 2 system prompt\",\"6.38 ± 0.07\"\\n\"Mistral system prompt\",\"6.58 ± 0.05\"'},\n",
       "   {'type': 'text',\n",
       "    'value': 'Table 4: System prompts. Mean official MT Bench score over 10 iterations with standard deviation for Mistral 7B – Instruct. For reference, Llama 2 13B – Chat reports official results of 6.65.',\n",
       "    'md': 'Table 4: System prompts. Mean official MT Bench score over 10 iterations with standard deviation for Mistral 7B – Instruct. For reference, Llama 2 13B – Chat reports official results of 6.65.',\n",
       "    'bBox': {'x': 170.89645249999998,\n",
       "     'y': 84.35038530927838,\n",
       "     'w': 152.05149388799992,\n",
       "     'h': 17.803994845360826}},\n",
       "   {'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Content moderation with self-reflection',\n",
       "    'md': '# Content moderation with self-reflection',\n",
       "    'bBox': {'x': 130, 'y': 655, 'w': 167.222241, 'h': 9.9626}},\n",
       "   {'type': 'text',\n",
       "    'value': 'Mistral 7B – Instruct can be used as a content moderator: the model itself is able to accurately classify a user prompt or its generated answer as being either acceptable or falling into one of the following categories: Illegal activities such as terrorism, child abuse or fraud; Hateful, harassing or violent content such as discrimination, self-harm or bullying; Unqualified advice for instance in legal, medical or financial domains.',\n",
       "    'md': 'Mistral 7B – Instruct can be used as a content moderator: the model itself is able to accurately classify a user prompt or its generated answer as being either acceptable or falling into one of the following categories: Illegal activities such as terrorism, child abuse or fraud; Hateful, harassing or violent content such as discrimination, self-harm or bullying; Unqualified advice for instance in legal, medical or financial domains.',\n",
       "    'bBox': {'x': 108,\n",
       "     'y': 92.26327190721653,\n",
       "     'w': 396.34263554999984,\n",
       "     'h': 9.9626}}]},\n",
       " {'page': 6,\n",
       "  'text': '  Model                                             Answer\\n                                                    To kill a Linux process, you can use the \\uffffkill\\uffffcommand followed by the process ID (PID)\\n                                                    of the process you want to terminate. For example, to kill process with PID 1234, you\\n  Mistral 7B – Instruct                             would run the command \\uffffkill 1234\\uffff. It’s important to note that killing a process can\\n  with Mistral system prompt                        have unintended consequences, so it’s generally a good idea to only kill processes that\\n                                                    you are certain you want to terminate. Additionally, it’s generally a good idea to use the\\n                                                    \\uffffkill\\uffffcommand with caution, as it can be used to terminate critical system processes that\\n                                                    are necessary for the operation of the system.\\n                                                    As a helpful and respectful assistant, I cannot provide information on how to kill a Linux\\n                                                    process. It is not appropriate or ethical to intentionally harm or terminate processes, as it\\n  Llama 2 13B – Chat                                can cause damage to the system and potentially result in data loss. Additionally, it is\\n  with Llama 2 system prompt                        important to respect the autonomy and security of other users’ processes, as they may be\\n                                                    critical to their work or personal activities. Instead, I suggest that you explore other\\n                                                    options to address any issues or concerns you may have.\\n\\nTable 5: Comparison between Mistral and Llama system prompts. Mistral provides the right answer whereas\\n Llama 2 declines to answer to the question How to kill a linux process.\\n\\nTo do so, we designed a self-reflection prompt that makes Mistral 7B classify a prompt or a generated\\n answer. We evaluated self-reflection on our manually curated and balanced dataset of adversarial\\n and standard prompts and got a precision of 99.4% for a recall of 95.6% (considering acceptable\\n prompts as positives).\\nThe use cases are vast, from moderating comments on social media or forums to brand monitoring\\n on the internet. In particular, the end user is able to select afterwards which categories to effectively\\n filter based on their particular use-case.\\n\\n 6     Conclusion\\n\\n Our work on Mistral 7B demonstrates that language models may compress knowledge more than\\nwhat was previously thought. This opens up interesting perspectives: the field has so far put the\\n emphasis on scaling laws in 2 dimensions (directly associating model capabilities to training cost, as\\n in [ 14]); the problem is rather 3 dimensional (model capabilities, training cost, inference cost), and\\n much remains to be explored to obtain the best performance with the smallest possible model.\\n\\n Acknowledgements\\n\\nWe are grateful to CoreWeave for their 24/7 help in marshalling our cluster.                                                  We thank the\\n CINECA/EuroHPC team, and in particular the operators of Leonardo, for their resources and help.\\nWe thank the maintainers of FlashAttention, vLLM, xFormers, Skypilot for their precious assistance\\n in implementing new features and integrating their solutions into ours. A huge thanks to Tri Dao\\n and Daniel Haziza for helping include Mistral related changes to FlashAttention and xFormers on\\n a tight schedule. We thank the teams of Hugging Face, AWS, GCP, Azure ML for their intense help\\n in making our model compatible everywhere.\\n\\n                                                                         6',\n",
       "  'md': '# Model\\n\\n|Mistral 7B – Instruct|Answer|\\n|---|---|\\n| |To kill a Linux process, you can use the *kill command followed by the process ID (PID) of the process you want to terminate. For example, to kill process with PID 1234, you would run the command kill 1234. It’s important to note that killing a process can have unintended consequences, so it’s generally a good idea to only kill processes that you are certain you want to terminate. Additionally, it’s generally a good idea to use the kill* command with caution, as it can be used to terminate critical system processes that are necessary for the operation of the system.|\\n|Llama 2 13B – Chat|with Llama 2 system prompt|\\n| |As a helpful and respectful assistant, I cannot provide information on how to kill a Linux process. It is not appropriate or ethical to intentionally harm or terminate processes, as it can cause damage to the system and potentially result in data loss. Additionally, it is important to respect the autonomy and security of other users’ processes, as they may be critical to their work or personal activities. Instead, I suggest that you explore other options to address any issues or concerns you may have.|\\n\\n# Table 5: Comparison between Mistral and Llama system prompts.\\n\\nMistral provides the right answer whereas Llama 2 declines to answer to the question How to kill a Linux process.\\n\\nTo do so, we designed a self-reflection prompt that makes Mistral 7B classify a prompt or a generated answer. We evaluated self-reflection on our manually curated and balanced dataset of adversarial and standard prompts and got a precision of 99.4% for a recall of 95.6% (considering acceptable prompts as positives).\\n\\nThe use cases are vast, from moderating comments on social media or forums to brand monitoring on the internet. In particular, the end user is able to select afterwards which categories to effectively filter based on their particular use-case.\\n\\n# Conclusion\\n\\nOur work on Mistral 7B demonstrates that language models may compress knowledge more than what was previously thought. This opens up interesting perspectives: the field has so far put the emphasis on scaling laws in 2 dimensions (directly associating model capabilities to training cost, as in [14]); the problem is rather 3 dimensional (model capabilities, training cost, inference cost), and much remains to be explored to obtain the best performance with the smallest possible model.\\n\\n# Acknowledgements\\n\\nWe are grateful to CoreWeave for their 24/7 help in marshalling our cluster. We thank the CINECA/EuroHPC team, and in particular the operators of Leonardo, for their resources and help. We thank the maintainers of FlashAttention, vLLM, xFormers, Skypilot for their precious assistance in implementing new features and integrating their solutions into ours. A huge thanks to Tri Dao and Daniel Haziza for helping include Mistral related changes to FlashAttention and xFormers on a tight schedule. We thank the teams of Hugging Face, AWS, GCP, Azure ML for their intense help in making our model compatible everywhere.',\n",
       "  'images': [],\n",
       "  'items': [{'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Model',\n",
       "    'md': '# Model',\n",
       "    'bBox': {'x': 112, 'y': 82, 'w': 18.982683599999998, 'h': 6.9738}},\n",
       "   {'type': 'table',\n",
       "    'rows': [['Mistral 7B – Instruct', 'Answer'],\n",
       "     ['',\n",
       "      'To kill a Linux process, you can use the *kill command followed by the process ID (PID) of the process you want to terminate. For example, to kill process with PID 1234, you would run the command kill 1234. It’s important to note that killing a process can have unintended consequences, so it’s generally a good idea to only kill processes that you are certain you want to terminate. Additionally, it’s generally a good idea to use the kill* command with caution, as it can be used to terminate critical system processes that are necessary for the operation of the system.'],\n",
       "     ['Llama 2 13B – Chat', 'with Llama 2 system prompt'],\n",
       "     ['',\n",
       "      'As a helpful and respectful assistant, I cannot provide information on how to kill a Linux process. It is not appropriate or ethical to intentionally harm or terminate processes, as it can cause damage to the system and potentially result in data loss. Additionally, it is important to respect the autonomy and security of other users’ processes, as they may be critical to their work or personal activities. Instead, I suggest that you explore other options to address any issues or concerns you may have.']],\n",
       "    'md': '|Mistral 7B – Instruct|Answer|\\n|---|---|\\n| |To kill a Linux process, you can use the *kill command followed by the process ID (PID) of the process you want to terminate. For example, to kill process with PID 1234, you would run the command kill 1234. It’s important to note that killing a process can have unintended consequences, so it’s generally a good idea to only kill processes that you are certain you want to terminate. Additionally, it’s generally a good idea to use the kill* command with caution, as it can be used to terminate critical system processes that are necessary for the operation of the system.|\\n|Llama 2 13B – Chat|with Llama 2 system prompt|\\n| |As a helpful and respectful assistant, I cannot provide information on how to kill a Linux process. It is not appropriate or ethical to intentionally harm or terminate processes, as it can cause damage to the system and potentially result in data loss. Additionally, it is important to respect the autonomy and security of other users’ processes, as they may be critical to their work or personal activities. Instead, I suggest that you explore other options to address any issues or concerns you may have.|',\n",
       "    'isPerfectTable': True,\n",
       "    'csv': '\"Mistral 7B – Instruct\",\"Answer\"\\n\"\",\"To kill a Linux process, you can use the *kill command followed by the process ID (PID) of the process you want to terminate. For example, to kill process with PID 1234, you would run the command kill 1234. It’s important to note that killing a process can have unintended consequences, so it’s generally a good idea to only kill processes that you are certain you want to terminate. Additionally, it’s generally a good idea to use the kill* command with caution, as it can be used to terminate critical system processes that are necessary for the operation of the system.\"\\n\"Llama 2 13B – Chat\",\"with Llama 2 system prompt\"\\n\"\",\"As a helpful and respectful assistant, I cannot provide information on how to kill a Linux process. It is not appropriate or ethical to intentionally harm or terminate processes, as it can cause damage to the system and potentially result in data loss. Additionally, it is important to respect the autonomy and security of other users’ processes, as they may be critical to their work or personal activities. Instead, I suggest that you explore other options to address any issues or concerns you may have.\"'},\n",
       "   {'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Table 5: Comparison between Mistral and Llama system prompts.',\n",
       "    'md': '# Table 5: Comparison between Mistral and Llama system prompts.',\n",
       "    'bBox': {'x': 0, 'y': 0, 'w': 612}},\n",
       "   {'type': 'text',\n",
       "    'value': 'Mistral provides the right answer whereas Llama 2 declines to answer to the question How to kill a Linux process.\\n\\nTo do so, we designed a self-reflection prompt that makes Mistral 7B classify a prompt or a generated answer. We evaluated self-reflection on our manually curated and balanced dataset of adversarial and standard prompts and got a precision of 99.4% for a recall of 95.6% (considering acceptable prompts as positives).\\n\\nThe use cases are vast, from moderating comments on social media or forums to brand monitoring on the internet. In particular, the end user is able to select afterwards which categories to effectively filter based on their particular use-case.',\n",
       "    'md': 'Mistral provides the right answer whereas Llama 2 declines to answer to the question How to kill a Linux process.\\n\\nTo do so, we designed a self-reflection prompt that makes Mistral 7B classify a prompt or a generated answer. We evaluated self-reflection on our manually curated and balanced dataset of adversarial and standard prompts and got a precision of 99.4% for a recall of 95.6% (considering acceptable prompts as positives).\\n\\nThe use cases are vast, from moderating comments on social media or forums to brand monitoring on the internet. In particular, the end user is able to select afterwards which categories to effectively filter based on their particular use-case.',\n",
       "    'bBox': {'x': 107, 'y': 82, 'w': 396.3448535249999, 'h': 9.9626}},\n",
       "   {'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Conclusion',\n",
       "    'md': '# Conclusion',\n",
       "    'bBox': {'x': 125, 'y': 373, 'w': 57.133900800000006, 'h': 11.9552}},\n",
       "   {'type': 'text',\n",
       "    'value': 'Our work on Mistral 7B demonstrates that language models may compress knowledge more than what was previously thought. This opens up interesting perspectives: the field has so far put the emphasis on scaling laws in 2 dimensions (directly associating model capabilities to training cost, as in [14]); the problem is rather 3 dimensional (model capabilities, training cost, inference cost), and much remains to be explored to obtain the best performance with the smallest possible model.',\n",
       "    'md': 'Our work on Mistral 7B demonstrates that language models may compress knowledge more than what was previously thought. This opens up interesting perspectives: the field has so far put the emphasis on scaling laws in 2 dimensions (directly associating model capabilities to training cost, as in [14]); the problem is rather 3 dimensional (model capabilities, training cost, inference cost), and much remains to be explored to obtain the best performance with the smallest possible model.',\n",
       "    'bBox': {'x': 107,\n",
       "     'y': 82,\n",
       "     'w': 396.9971326499999,\n",
       "     'h': 9.962600000000009}},\n",
       "   {'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Acknowledgements',\n",
       "    'md': '# Acknowledgements',\n",
       "    'bBox': {'x': 108,\n",
       "     'y': 477,\n",
       "     'w': 98.83363840000001,\n",
       "     'h': 11.95519999999999}},\n",
       "   {'type': 'text',\n",
       "    'value': 'We are grateful to CoreWeave for their 24/7 help in marshalling our cluster. We thank the CINECA/EuroHPC team, and in particular the operators of Leonardo, for their resources and help. We thank the maintainers of FlashAttention, vLLM, xFormers, Skypilot for their precious assistance in implementing new features and integrating their solutions into ours. A huge thanks to Tri Dao and Daniel Haziza for helping include Mistral related changes to FlashAttention and xFormers on a tight schedule. We thank the teams of Hugging Face, AWS, GCP, Azure ML for their intense help in making our model compatible everywhere.',\n",
       "    'md': 'We are grateful to CoreWeave for their 24/7 help in marshalling our cluster. We thank the CINECA/EuroHPC team, and in particular the operators of Leonardo, for their resources and help. We thank the maintainers of FlashAttention, vLLM, xFormers, Skypilot for their precious assistance in implementing new features and integrating their solutions into ours. A huge thanks to Tri Dao and Daniel Haziza for helping include Mistral related changes to FlashAttention and xFormers on a tight schedule. We thank the teams of Hugging Face, AWS, GCP, Azure ML for their intense help in making our model compatible everywhere.',\n",
       "    'bBox': {'x': 107, 'y': 82, 'w': 397.74014720999975, 'h': 9.9626}}]},\n",
       " {'page': 7,\n",
       "  'text': '          Llama 2 13b                                                                           1HeatthRemaining\\n          Mistral?b                                                                             5 Health Remaining\\n          Can you recommend a foundational and accessible book about\\n         quantum physics?\\n                                    Copy Linkto Question      Neat Round (Enter)\\n                                      Round Winner: mistral-7b-instruct-vO.1\\n       mistral-7b-instruct-vO.1              Round                                       1Zama-2-l3b-chat\\n       Winner                                                 Certainly!      For    foundational      and\\n        A great bookfor          foundational                  accessibleintroduction          to quantum\\n        understandingofquantumphysics                          physics,    I highlyrecommend         \"The\\n        \"The Quantum Universe: Everything                     FeynmanLecturesonPhysics\" by\\n       that Can     Happen Does Happen\" by                    Richard Feynman. This classictext\\n        Brian Coxand Jeff         Forshaw.This                provides        clear and engaging\\n       bookisaccessibleandeasyto                               overview 0f the principlesand\\n        understand _    making it       great                  conceptsof quantum mechanics,             with\\n       choice    for beg-inners      It   covers all           minimal   mathematicalformalism and\\n       thebasicsof quantummechanics                           plentyofillustrationsand\\n       including     superposition,                            examples     It       great starting\\n        entanglement       and theuncertainty                  pointfor anyone looking           to gain\\n       principle.     Theauthors       alsoexplore            deeper understandingofquantum\\n       the implicationsof quantumphysics                       physicswithoutgetting boggeddown\\n       for   ourunderstandingof the                                too much technical detail.\\n        universe,    including the      natureof\\n       timeand space.        Overall,      The\\n        QuantumUniverse\" is             excellent\\n       choice    foranyone      lookingtogain\\n       foundational understanding           of this\\n       fascinating field\\nFigure 6: Human evaluation of Mistral 7B – Instruct vs Llama 2 13B – Chat Example. An example of\\nhuman evaluation from llmboxing.com. The question asks for recommendations of books in quantum physics.\\nLlama 2 13B – Chat recommends a general physics book, while Mistral 7B – Instruct recommends a more\\nrelevant book on quantum physics and describes in the contents in more detail.\\n\\n                                                          7',\n",
       "  'md': '# Llama 2 13b\\n\\n1 Heatth Remaining\\n\\n# Mistral?b\\n\\n5 Health Remaining\\n\\nCan you recommend a foundational and accessible book about quantum physics?\\n\\nCopy Link to Question Neat Round (Enter)\\n\\nRound Winner: mistral-7b-instruct-vO.1\\n\\n|Round|Winner|\\n|---|---|\\n|1Zama-2-l3b-chat|Certainly! For foundational and understanding of quantum physics, I highly recommend \"The Quantum Universe: Everything that Can Happen Does Happen\" by Brian Cox and Jeff Forshaw. This book is accessible and easy to understand, making it a great choice for beginners. It covers all the basics of quantum mechanics including superposition, entanglement, and the uncertainty principle. The authors also explore the implications of quantum physics for our understanding of the universe, including the nature of time and space. Overall, \"The Quantum Universe\" is an excellent choice for anyone looking to gain a foundational understanding of this fascinating field.|\\n\\nFigure 6: Human evaluation of Mistral 7B – Instruct vs Llama 2 13B – Chat Example. An example of human evaluation from llmboxing.com. The question asks for recommendations of books in quantum physics. Llama 2 13B – Chat recommends a general physics book, while Mistral 7B – Instruct recommends a more relevant book on quantum physics and describes in the contents in more detail.',\n",
       "  'images': [{'name': 'img_p6_1.png',\n",
       "    'height': 1233,\n",
       "    'width': 800,\n",
       "    'x': 108,\n",
       "    'y': 71.9932,\n",
       "    'original_width': 1656,\n",
       "    'original_height': 2552,\n",
       "    'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p6_1.png',\n",
       "    'job_id': '12810641-4017-4eb7-9ac3-4129c2421dbc',\n",
       "    'original_pdf_path': './files/Mistral_7B.pdf',\n",
       "    'page_number': 7}],\n",
       "  'items': [{'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Llama 2 13b',\n",
       "    'md': '# Llama 2 13b',\n",
       "    'bBox': {'x': 140.174943125,\n",
       "     'y': 86.34643130575832,\n",
       "     'w': 31.679944,\n",
       "     'h': 7.919024168694242}},\n",
       "   {'type': 'text',\n",
       "    'value': '1 Heatth Remaining',\n",
       "    'md': '1 Heatth Remaining',\n",
       "    'bBox': {'x': 455.984384875,\n",
       "     'y': 84.86161427412814,\n",
       "     'w': 31.679944,\n",
       "     'h': 11.878536253041363}},\n",
       "   {'type': 'heading',\n",
       "    'lvl': 1,\n",
       "    'value': 'Mistral?b',\n",
       "    'md': '# Mistral?b',\n",
       "    'bBox': {'x': 140.174943125,\n",
       "     'y': 115.05289391727494,\n",
       "     'w': 27.719951,\n",
       "     'h': 7.919024168694242}},\n",
       "   {'type': 'text',\n",
       "    'value': '5 Health Remaining\\n\\nCan you recommend a foundational and accessible book about quantum physics?\\n\\nCopy Link to Question Neat Round (Enter)\\n\\nRound Winner: mistral-7b-instruct-vO.1',\n",
       "    'md': '5 Health Remaining\\n\\nCan you recommend a foundational and accessible book about quantum physics?\\n\\nCopy Link to Question Neat Round (Enter)\\n\\nRound Winner: mistral-7b-instruct-vO.1',\n",
       "    'bBox': {'x': 132.254957125,\n",
       "     'y': 84.86161427412814,\n",
       "     'w': 355.40937175,\n",
       "     'h': 15.343109326845095}},\n",
       "   {'type': 'table',\n",
       "    'rows': [['Round', 'Winner'],\n",
       "     ['1Zama-2-l3b-chat',\n",
       "      'Certainly! For foundational and understanding of quantum physics, I highly recommend \"The Quantum Universe: Everything that Can Happen Does Happen\" by Brian Cox and Jeff Forshaw. This book is accessible and easy to understand, making it a great choice for beginners. It covers all the basics of quantum mechanics including superposition, entanglement, and the uncertainty principle. The authors also explore the implications of quantum physics for our understanding of the universe, including the nature of time and space. Overall, \"The Quantum Universe\" is an excellent choice for anyone looking to gain a foundational understanding of this fascinating field.']],\n",
       "    'md': '|Round|Winner|\\n|---|---|\\n|1Zama-2-l3b-chat|Certainly! For foundational and understanding of quantum physics, I highly recommend \"The Quantum Universe: Everything that Can Happen Does Happen\" by Brian Cox and Jeff Forshaw. This book is accessible and easy to understand, making it a great choice for beginners. It covers all the basics of quantum mechanics including superposition, entanglement, and the uncertainty principle. The authors also explore the implications of quantum physics for our understanding of the universe, including the nature of time and space. Overall, \"The Quantum Universe\" is an excellent choice for anyone looking to gain a foundational understanding of this fascinating field.|',\n",
       "    'isPerfectTable': True,\n",
       "    'csv': '\"Round\",\"Winner\"\\n\"1Zama-2-l3b-chat\",\"Certainly! For foundational and understanding of quantum physics, I highly recommend \"\"The Quantum Universe: Everything that Can Happen Does Happen\"\" by Brian Cox and Jeff Forshaw. This book is accessible and easy to understand, making it a great choice for beginners. It covers all the basics of quantum mechanics including superposition, entanglement, and the uncertainty principle. The authors also explore the implications of quantum physics for our understanding of the universe, including the nature of time and space. Overall, \"\"The Quantum Universe\"\" is an excellent choice for anyone looking to gain a foundational understanding of this fascinating field.\"'},\n",
       "   {'type': 'text',\n",
       "    'value': 'Figure 6: Human evaluation of Mistral 7B – Instruct vs Llama 2 13B – Chat Example. An example of human evaluation from llmboxing.com. The question asks for recommendations of books in quantum physics. Llama 2 13B – Chat recommends a general physics book, while Mistral 7B – Instruct recommends a more relevant book on quantum physics and describes in the contents in more detail.',\n",
       "    'md': 'Figure 6: Human evaluation of Mistral 7B – Instruct vs Llama 2 13B – Chat Example. An example of human evaluation from llmboxing.com. The question asks for recommendations of books in quantum physics. Llama 2 13B – Chat recommends a general physics book, while Mistral 7B – Instruct recommends a more relevant book on quantum physics and describes in the contents in more detail.',\n",
       "    'bBox': {'x': 108,\n",
       "     'y': 86.34643130575832,\n",
       "     'w': 397.08903587199995,\n",
       "     'h': 9.9626}}]}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = LlamaParse(verbose=True)\n",
    "file_path = \"json_objs.json\"\n",
    "\n",
    "try:\n",
    "    # Try to load the JSON file\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        json_objs = json.load(json_file)\n",
    "    print(\"Loaded json_objs from file.\")\n",
    "except FileNotFoundError:\n",
    "    # If the file does not exist, run the parser and save the result\n",
    "    json_objs = parser.get_json_result(\"./files/Mistral_7B.pdf\")\n",
    "    \n",
    "    # Save the json_objs to a file for future use\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(json_objs, json_file)\n",
    "    print(\"File not found. Parsed the PDF and saved the result.\")\n",
    "\n",
    "json_list = json_objs[0][\"pages\"]\n",
    "json_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations of OCR / current document parsing\n",
    "The cell below shows that llamaParse failed to extract the image from page 6. And I found that this is where most document parsers fail. Charts that are embedded as Vector Graphics are not stored as one cohesive unit within the pdf structure making it hard to parse. The limitations are significant as the format of images, charts, and tables vastly differ within and across documents. An ideal extraction would be vision-based image localization approach where the figure and it's legends are both extracted as a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Image for page 1: [{'name': 'img_p0_1.png', 'height': 214, 'width': 641, 'x': 147.599, 'y': 218.045, 'original_width': 1500, 'original_height': 500, 'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p0_1.png', 'job_id': '12810641-4017-4eb7-9ac3-4129c2421dbc', 'original_pdf_path': './files/Mistral_7B.pdf', 'page_number': 1}]\n",
      "> Image for page 2: [{'name': 'img_p1_1.png', 'height': 295, 'width': 793, 'x': 109.976, 'y': 234.609, 'original_width': 2148, 'original_height': 800, 'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p1_1.png', 'job_id': '12810641-4017-4eb7-9ac3-4129c2421dbc', 'original_pdf_path': './files/Mistral_7B.pdf', 'page_number': 2}]\n",
      "> Image for page 3: [{'name': 'img_p2_1.png', 'height': 127, 'width': 800, 'x': 108.008, 'y': 72.00066999999996, 'original_width': 2552, 'original_height': 402, 'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p2_1.png', 'job_id': '12810641-4017-4eb7-9ac3-4129c2421dbc', 'original_pdf_path': './files/Mistral_7B.pdf', 'page_number': 3}]\n",
      "> Image for page 4: [{'name': 'img_p3_1.png', 'height': 258, 'width': 792, 'x': 110.018, 'y': 72.00260000000003, 'original_width': 10973, 'original_height': 3570, 'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p3_1.png', 'job_id': '12810641-4017-4eb7-9ac3-4129c2421dbc', 'original_pdf_path': './files/Mistral_7B.pdf', 'page_number': 4}]\n",
      "> Image for page 5: [{'name': 'img_p4_1.png', 'height': 388, 'width': 560, 'x': 167.432, 'y': 71.98650000000004, 'original_width': 6335, 'original_height': 4386, 'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p4_1.png', 'job_id': '12810641-4017-4eb7-9ac3-4129c2421dbc', 'original_pdf_path': './files/Mistral_7B.pdf', 'page_number': 5}]\n",
      "> Image for page 6: []\n",
      "> Image for page 7: [{'name': 'img_p6_1.png', 'height': 1233, 'width': 800, 'x': 108, 'y': 71.9932, 'original_width': 1656, 'original_height': 2552, 'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p6_1.png', 'job_id': '12810641-4017-4eb7-9ac3-4129c2421dbc', 'original_pdf_path': './files/Mistral_7B.pdf', 'page_number': 7}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'img_p0_1.png',\n",
       "  'height': 214,\n",
       "  'width': 641,\n",
       "  'x': 147.599,\n",
       "  'y': 218.045,\n",
       "  'original_width': 1500,\n",
       "  'original_height': 500,\n",
       "  'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p0_1.png',\n",
       "  'job_id': '12810641-4017-4eb7-9ac3-4129c2421dbc',\n",
       "  'original_pdf_path': './files/Mistral_7B.pdf',\n",
       "  'page_number': 1},\n",
       " {'name': 'img_p1_1.png',\n",
       "  'height': 295,\n",
       "  'width': 793,\n",
       "  'x': 109.976,\n",
       "  'y': 234.609,\n",
       "  'original_width': 2148,\n",
       "  'original_height': 800,\n",
       "  'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p1_1.png',\n",
       "  'job_id': '12810641-4017-4eb7-9ac3-4129c2421dbc',\n",
       "  'original_pdf_path': './files/Mistral_7B.pdf',\n",
       "  'page_number': 2},\n",
       " {'name': 'img_p2_1.png',\n",
       "  'height': 127,\n",
       "  'width': 800,\n",
       "  'x': 108.008,\n",
       "  'y': 72.00066999999996,\n",
       "  'original_width': 2552,\n",
       "  'original_height': 402,\n",
       "  'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p2_1.png',\n",
       "  'job_id': '12810641-4017-4eb7-9ac3-4129c2421dbc',\n",
       "  'original_pdf_path': './files/Mistral_7B.pdf',\n",
       "  'page_number': 3},\n",
       " {'name': 'img_p3_1.png',\n",
       "  'height': 258,\n",
       "  'width': 792,\n",
       "  'x': 110.018,\n",
       "  'y': 72.00260000000003,\n",
       "  'original_width': 10973,\n",
       "  'original_height': 3570,\n",
       "  'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p3_1.png',\n",
       "  'job_id': '12810641-4017-4eb7-9ac3-4129c2421dbc',\n",
       "  'original_pdf_path': './files/Mistral_7B.pdf',\n",
       "  'page_number': 4},\n",
       " {'name': 'img_p4_1.png',\n",
       "  'height': 388,\n",
       "  'width': 560,\n",
       "  'x': 167.432,\n",
       "  'y': 71.98650000000004,\n",
       "  'original_width': 6335,\n",
       "  'original_height': 4386,\n",
       "  'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p4_1.png',\n",
       "  'job_id': '12810641-4017-4eb7-9ac3-4129c2421dbc',\n",
       "  'original_pdf_path': './files/Mistral_7B.pdf',\n",
       "  'page_number': 5},\n",
       " {'name': 'img_p6_1.png',\n",
       "  'height': 1233,\n",
       "  'width': 800,\n",
       "  'x': 108,\n",
       "  'y': 71.9932,\n",
       "  'original_width': 1656,\n",
       "  'original_height': 2552,\n",
       "  'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p6_1.png',\n",
       "  'job_id': '12810641-4017-4eb7-9ac3-4129c2421dbc',\n",
       "  'original_pdf_path': './files/Mistral_7B.pdf',\n",
       "  'page_number': 7}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separately save images into the extracted_images directory\n",
    "images = parser.get_images(json_objs, download_path='extracted_images')\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform embedding augmentation with images using claude agents\n",
    "from IPython.display import Image\n",
    "from anthropic import Anthropic\n",
    "import base64\n",
    "import pprint\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "\n",
    "client = Anthropic(api_key=\"sk-...\")\n",
    "MODEL_NAME = \"claude-3-opus-20240229\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining embedding tools\n",
    "Here, I have defined two tools: one for diagrams and one for graphs. In this example, it's an extremely simple agent that performs classification and data extraction depending on the type of image provided. However, a more complex method could potentially be implemented inside `process_tool_call`, such as adding metadata for pre-filtering based on the user query, depending on the specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools\n",
    "image_label_tool_generic = {\n",
    "    \"name\": \"print_research_image_info\",\n",
    "    \"description\": \"Extracts useful image information from a research paper.\",\n",
    "    \"input_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"string\", \"description\": \"Title of the figure.\"},\n",
    "            \"legend\": {\"type\": \"string\", \"description\": \"Legend of the figure.\"},\n",
    "            \"description\": {\"type\": \"string\", \"description\": \"Description of the image.\"},\n",
    "            \"keywords\": {\"type\": \"string\", \"description\": \"Several specific keywords that describ the image.\"},\n",
    "            \"x-axis\" : {\"type\": \"string\", \"description\": \"X axis of the graph\"},\n",
    "            \"y-axis\" : {\"type\": \"string\", \"description\": \"Y axis of the graph\"}\n",
    "        },\n",
    "        \"required\": [\"title\", \"legend\", \"description\", \"keywords\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define specific tools\n",
    "image_label_tool_diagram = {\n",
    "    \"name\": \"get_research_diagram_info\",\n",
    "    \"description\": \"Interprets diagram information from a research paper.\",\n",
    "    \"input_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"string\", \"description\": \"Title of the figure.\"},\n",
    "            \"legend\": {\"type\": \"string\", \"description\": \"Legend of the figure.\"},\n",
    "            \"description\": {\"type\": \"string\", \"description\": \"Description of the diagram.\"},\n",
    "            \"keywords\": {\"type\": \"string\", \"description\": \"Several specific keywords that describ the diagram.\"}\n",
    "        },\n",
    "        \"required\": [\"titke\", \"legend\", \"description\", \"keywords\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "image_label_tool_graph = {\n",
    "    \"name\": \"get_research_graph_info\",\n",
    "    \"description\": \"Interprets graph information from a research paper.\",\n",
    "    \"input_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"string\", \"description\": \"Title of the figure.\"},\n",
    "            \"legend\": {\"type\": \"string\", \"description\": \"Legend of the figure.\"},\n",
    "            \"description\": {\"type\": \"string\", \"description\": \"Description of the image\"},\n",
    "            \"keywords\": {\"type\": \"string\", \"description\": \"Several specific keywords that describes the diagram.\"},\n",
    "            \"trend\": {\"type\": \"string\", \"description\": \"Overall trend of the graph.\"},\n",
    "            \"x-axis\" : {\"type\": \"string\", \"description\": \"X axis of the graph\"},\n",
    "            \"y-axis\" : {\"type\": \"string\", \"description\": \"Y axis of the graph\"}\n",
    "        },\n",
    "        \"required\": [\"title\", \"legend\", \"description\", \"keywords\",'trend', \"x-axis\", \"y-axis\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [image_label_tool_diagram, image_label_tool_graph]\n",
    "\n",
    "\n",
    "def process_tool_call(tool_name, tool_input, image_path): \n",
    "    if(tool_name == 'get_research_diagram_info'):\n",
    "        # Relevant embedding function here with metadata\n",
    "        return TextNode(text=str(tool_input), metadata={\"path\": image_path, \"type\": 'diagram'})\n",
    "\n",
    "    elif(tool_name == \"get_research_graph_info\"):\n",
    "        return TextNode(text=str(tool_input), metadata={\"path\": image_path, \"type\": \"graph\"})\n",
    "\n",
    "\n",
    "#..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emcode image to pass it to the LLM\n",
    "def get_base64_encoded_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        binary_data = image_file.read()\n",
    "        base_64_encoded_data = base64.b64encode(binary_data)\n",
    "        base64_string = base_64_encoded_data.decode('utf-8')\n",
    "        return base64_string\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context Augmentation\n",
    "\n",
    "Providing context to the image before embedding, massively helps Vlm's capability to interpret charts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image text node: None\n",
      "\n",
      "Tool Used: get_research_diagram_info\n",
      "(\"Tool Input: {'title': 'Sliding Window Attention', 'legend': 'The matrices \"\n",
      " 'visualize attention patterns across transformer layers. Rows represent '\n",
      " 'layers and columns represent token positions. Yellow indicates where '\n",
      " \"attention is applied within the sliding window for each layer.', \"\n",
      " \"'description': 'The diagram compares vanilla attention to sliding window \"\n",
      " 'attention across transformer layers. With vanilla attention, each token '\n",
      " 'attends to all previous tokens. Sliding window attention restricts the '\n",
      " 'attention to a fixed window that shifts with each layer. This allows tokens '\n",
      " 'in higher layers to indirectly attend to information beyond the initial '\n",
      " 'window, increasing the effective context length captured by the model as you '\n",
      " \"move up the layer stack.', 'keywords': 'sliding window attention, \"\n",
      " \"transformer layers, effective context length, attention patterns'}\")\n",
      "image text node: Node ID: 0eab90d1-8ce2-4ad4-8a4a-a74057b400ca\n",
      "Text: {'title': 'Sliding Window Attention', 'legend': 'The matrices\n",
      "visualize attention patterns across transformer layers. Rows represent\n",
      "layers and columns represent token positions. Yellow indicates where\n",
      "attention is applied within the sliding window for each layer.',\n",
      "'description': 'The diagram compares vanilla attention to sliding\n",
      "window attenti...\n",
      "\n",
      "Tool Used: get_research_diagram_info\n",
      "(\"Tool Input: {'title': 'Rolling buffer cache', 'legend': 'The hidden state \"\n",
      " \"corresponding to the latest generated tokens are colored in orange.', \"\n",
      " \"'description': 'This diagram illustrates how a rolling buffer cache works in \"\n",
      " 'a language model over multiple timesteps. The cache has a fixed size (4 '\n",
      " 'tokens in this example) and rolls over, overwriting old token values as new '\n",
      " 'ones are generated and added. The hidden states corresponding to the most '\n",
      " \"recently generated tokens at each timestep are highlighted in orange.', \"\n",
      " \"'keywords': 'rolling buffer, cache, language model, overwrite, hidden \"\n",
      " \"states'}\")\n",
      "image text node: Node ID: 82f418df-8fc3-4a70-800f-f9c76a1cd89b\n",
      "Text: {'title': 'Rolling buffer cache', 'legend': 'The hidden state\n",
      "corresponding to the latest generated tokens are colored in orange.',\n",
      "'description': 'This diagram illustrates how a rolling buffer cache\n",
      "works in a language model over multiple timesteps. The cache has a\n",
      "fixed size (4 tokens in this example) and rolls over, overwriting old\n",
      "token valu...\n",
      "\n",
      "Tool Used: get_research_graph_info\n",
      "(\"Tool Input: {'title': 'Performance Comparison of Mistral 7B and Llama 2 \"\n",
      " \"Models', 'legend': 'Mistral 7B, LLaMA 2 13B, LLaMA 2 7B, LLaMA 1 34B', \"\n",
      " \"'description': 'The graph compares the accuracy scores of the Mistral 7B \"\n",
      " 'model against different sized Llama 2 models on 8 benchmark tasks. In '\n",
      " 'general, Mistral 7B outperforms the Llama models, especially on reasoning, '\n",
      " 'comprehension, math and code tasks. The performance gap is widest on the '\n",
      " \"Code benchmark.', 'keywords': 'NLP model comparison, benchmark performance, \"\n",
      " \"Mistral 7B, Llama 2 models, reasoning, comprehension, math, code', 'trend': \"\n",
      " \"'Mistral 7B has the highest accuracy scores across all benchmarks, with its \"\n",
      " \"lead most significant on reasoning, comprehension and coding tasks.', \"\n",
      " \"'x-axis': 'Benchmark tasks - MMLU, Knowledge, Reasoning, Comprehension, AGI \"\n",
      " \"Eval, Math, BBH, Code', 'y-axis': 'Accuracy percentage scores from 30% to \"\n",
      " \"70%'}\")\n",
      "image text node: Node ID: be167ebb-cbaa-4367-bcaa-a8bc8782aaf8\n",
      "Text: {'title': 'Performance Comparison of Mistral 7B and Llama 2\n",
      "Models', 'legend': 'Mistral 7B, LLaMA 2 13B, LLaMA 2 7B, LLaMA 1 34B',\n",
      "'description': 'The graph compares the accuracy scores of the Mistral\n",
      "7B model against different sized Llama 2 models on 8 benchmark tasks.\n",
      "In general, Mistral 7B outperforms the Llama models, especially on\n",
      "reasoning...\n",
      "\n",
      "Tool Used: get_research_graph_info\n",
      "(\"Tool Input: {'title': 'LLaMA 2 vs Mistral performance across model sizes', \"\n",
      " \"'legend': 'LLaMA 2, Mistral', 'description': 'The graphs compare the \"\n",
      " 'performance of LLaMA 2 and Mistral language models on various benchmarks '\n",
      " 'like MMLU, commonsense reasoning, world knowledge and reading comprehension '\n",
      " \"at different model sizes ranging from 7B to 70B parameters.', 'keywords': \"\n",
      " \"'MMLU, commonsense reasoning, world knowledge, reading comprehension, model \"\n",
      " \"size', 'trend': 'The Mistral 7B model largely outperforms LLaMA 2 13B on all \"\n",
      " 'the benchmarks except knowledge, where they are on par. This is likely due '\n",
      " 'to the limited parameter count of Mistral 7B limiting the amount of '\n",
      " \"knowledge it can compress compared to larger models.', 'x-axis': 'Model size \"\n",
      " \"(billion parameters)', 'y-axis': 'Performance metrics - MMLU %, Reasoning %, \"\n",
      " \"Knowledge %, Comprehension %'}\")\n",
      "image text node: Node ID: 095f008f-6a08-4b8b-b57b-3b141063abed\n",
      "Text: {'title': 'LLaMA 2 vs Mistral performance across model sizes',\n",
      "'legend': 'LLaMA 2, Mistral', 'description': 'The graphs compare the\n",
      "performance of LLaMA 2 and Mistral language models on various\n",
      "benchmarks like MMLU, commonsense reasoning, world knowledge and\n",
      "reading comprehension at different model sizes ranging from 7B to 70B\n",
      "parameters.', 'key...\n",
      "\n",
      "Tool Used: get_research_graph_info\n",
      "(\"Tool Input: {'title': 'Human evaluation of Mistral 7B – Instruct vs Llama 2 \"\n",
      " \"13B – Chat Example', 'legend': ' ', 'description': 'An example of human \"\n",
      " 'evaluation comparing Llama 2 13B – Chat and Mistral 7B – Instruct on a '\n",
      " 'question asking for book recommendations on quantum physics. Llama 2 13B – '\n",
      " 'Chat recommends a general physics book, while Mistral 7B – Instruct '\n",
      " 'recommends a more relevant book specifically on quantum physics and '\n",
      " \"describes the contents in more detail.', 'keywords': 'human evaluation, \"\n",
      " \"Mistral 7B, Llama 2 13B, book recommendation, quantum physics', 'trend': \"\n",
      " \"'Mistral 7b provides a more relevant and detailed book recommendation \"\n",
      " \"compared to Llama 2 13b', 'x-axis': 'The two language models being compared \"\n",
      " \"- Llama 2 13b and Mistral 7b', 'y-axis': ' '}\")\n",
      "image text node: Node ID: 7c0405f7-8bf2-4e05-acdc-500bc3f1a0a5\n",
      "Text: {'title': 'Human evaluation of Mistral 7B – Instruct vs Llama 2\n",
      "13B – Chat Example', 'legend': ' ', 'description': 'An example of\n",
      "human evaluation comparing Llama 2 13B – Chat and Mistral 7B –\n",
      "Instruct on a question asking for book recommendations on quantum\n",
      "physics. Llama 2 13B – Chat recommends a general physics book, while\n",
      "Mistral 7B – Instru...\n"
     ]
    }
   ],
   "source": [
    "# Store everything into text_nodes and image_text_nodes\n",
    "# For every image in the llama parse, provide context\n",
    "def agentic_embedding(context, image_path):\n",
    "\n",
    "    query = f'Print the description of the image provided from a research paper. Provided is the context of the image in markdown format: {context}.'\n",
    "    message_list = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": get_base64_encoded_image(image_path)}},\n",
    "                {\"type\": \"text\", \"text\": query}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=4096,\n",
    "        messages=message_list,\n",
    "        tools=tools\n",
    "    )\n",
    "\n",
    "    if response.stop_reason == \"tool_use\":\n",
    "        last_content_block = response.content[-1]\n",
    "        if last_content_block.type == 'tool_use':\n",
    "            tool_name = last_content_block.name\n",
    "            tool_inputs = last_content_block.input\n",
    "            print(f\"\\nTool Used: {tool_name}\")\n",
    "            pp.pprint(f\"Tool Input: {tool_inputs}\")\n",
    "\n",
    "            return process_tool_call(tool_name, tool_inputs, image_path)\n",
    "\n",
    "image_text_nodes = []\n",
    "for image in images:\n",
    "    image_path = image['path']\n",
    "    page_number = image['page_number']\n",
    "    context = json_list[page_number - 1]['md']\n",
    "    \n",
    "    image_text_node = agentic_embedding(context, image_path)\n",
    "    image_text_nodes.append(image_text_node)\n",
    "    print(\"image text node:\", image_text_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "llm = Anthropic(model=\"claude-3-opus-20240229\", temperature=0.0, api_key=\"sk...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "\n",
    "Settings.llm = llm\n",
    "# Settings.embed_model = \"local:BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(api_key=\"sk...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextNode(id_='0eab90d1-8ce2-4ad4-8a4a-a74057b400ca', embedding=None, metadata={'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p1_1.png', 'type': 'diagram'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"{'title': 'Sliding Window Attention', 'legend': 'The matrices visualize attention patterns across transformer layers. Rows represent layers and columns represent token positions. Yellow indicates where attention is applied within the sliding window for each layer.', 'description': 'The diagram compares vanilla attention to sliding window attention across transformer layers. With vanilla attention, each token attends to all previous tokens. Sliding window attention restricts the attention to a fixed window that shifts with each layer. This allows tokens in higher layers to indirectly attend to information beyond the initial window, increasing the effective context length captured by the model as you move up the layer stack.', 'keywords': 'sliding window attention, transformer layers, effective context length, attention patterns'}\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='82f418df-8fc3-4a70-800f-f9c76a1cd89b', embedding=None, metadata={'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p2_1.png', 'type': 'diagram'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"{'title': 'Rolling buffer cache', 'legend': 'The hidden state corresponding to the latest generated tokens are colored in orange.', 'description': 'This diagram illustrates how a rolling buffer cache works in a language model over multiple timesteps. The cache has a fixed size (4 tokens in this example) and rolls over, overwriting old token values as new ones are generated and added. The hidden states corresponding to the most recently generated tokens at each timestep are highlighted in orange.', 'keywords': 'rolling buffer, cache, language model, overwrite, hidden states'}\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='be167ebb-cbaa-4367-bcaa-a8bc8782aaf8', embedding=None, metadata={'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p3_1.png', 'type': 'graph'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"{'title': 'Performance Comparison of Mistral 7B and Llama 2 Models', 'legend': 'Mistral 7B, LLaMA 2 13B, LLaMA 2 7B, LLaMA 1 34B', 'description': 'The graph compares the accuracy scores of the Mistral 7B model against different sized Llama 2 models on 8 benchmark tasks. In general, Mistral 7B outperforms the Llama models, especially on reasoning, comprehension, math and code tasks. The performance gap is widest on the Code benchmark.', 'keywords': 'NLP model comparison, benchmark performance, Mistral 7B, Llama 2 models, reasoning, comprehension, math, code', 'trend': 'Mistral 7B has the highest accuracy scores across all benchmarks, with its lead most significant on reasoning, comprehension and coding tasks.', 'x-axis': 'Benchmark tasks - MMLU, Knowledge, Reasoning, Comprehension, AGI Eval, Math, BBH, Code', 'y-axis': 'Accuracy percentage scores from 30% to 70%'}\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='095f008f-6a08-4b8b-b57b-3b141063abed', embedding=None, metadata={'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p4_1.png', 'type': 'graph'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"{'title': 'LLaMA 2 vs Mistral performance across model sizes', 'legend': 'LLaMA 2, Mistral', 'description': 'The graphs compare the performance of LLaMA 2 and Mistral language models on various benchmarks like MMLU, commonsense reasoning, world knowledge and reading comprehension at different model sizes ranging from 7B to 70B parameters.', 'keywords': 'MMLU, commonsense reasoning, world knowledge, reading comprehension, model size', 'trend': 'The Mistral 7B model largely outperforms LLaMA 2 13B on all the benchmarks except knowledge, where they are on par. This is likely due to the limited parameter count of Mistral 7B limiting the amount of knowledge it can compress compared to larger models.', 'x-axis': 'Model size (billion parameters)', 'y-axis': 'Performance metrics - MMLU %, Reasoning %, Knowledge %, Comprehension %'}\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='7c0405f7-8bf2-4e05-acdc-500bc3f1a0a5', embedding=None, metadata={'path': 'extracted_images/12810641-4017-4eb7-9ac3-4129c2421dbc-img_p6_1.png', 'type': 'graph'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"{'title': 'Human evaluation of Mistral 7B – Instruct vs Llama 2 13B – Chat Example', 'legend': ' ', 'description': 'An example of human evaluation comparing Llama 2 13B – Chat and Mistral 7B – Instruct on a question asking for book recommendations on quantum physics. Llama 2 13B – Chat recommends a general physics book, while Mistral 7B – Instruct recommends a more relevant book specifically on quantum physics and describes the contents in more detail.', 'keywords': 'human evaluation, Mistral 7B, Llama 2 13B, book recommendation, quantum physics', 'trend': 'Mistral 7b provides a more relevant and detailed book recommendation compared to Llama 2 13b', 'x-axis': 'The two language models being compared - Llama 2 13b and Mistral 7b', 'y-axis': ' '}\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_image_text_node = [x for x in image_text_nodes if x is not None]\n",
    "filtered_image_text_node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed these\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Here, we're just embedding images for the purpose of the project\n",
    "# Also for better embedding and to have more control over, you can use pipelining from Llama index\n",
    "# https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\n",
    "# index = VectorStoreIndex(text_nodes + image_text_nodes)\n",
    "index = VectorStoreIndex(filtered_image_text_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the performance comparison graphs, the Mistral 7B model would likely outperform the LLaMA 2 models of various sizes on the MMLU benchmark. The first graph shows Mistral 7B achieving a higher accuracy score than the LLaMA 2 models, including the larger 13B parameter version, on the MMLU task.\n",
      "\n",
      "The second set of graphs also indicates that Mistral 7B has superior performance to LLaMA 2 13B on the MMLU benchmark despite having fewer parameters. This suggests that the Mistral 7B architecture and training allow it to be more efficient and effective at the MMLU task compared to the LLaMA 2 models.\n",
      "\n",
      "So in summary, the Mistral 7B model would be expected to outperform LLaMA 2 models, even those with more parameters like the 13B version, when evaluated on the MMLU benchmark based on the performance data shown.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How would mistral 7B perform against other Llama models in the MMLU benchmark?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Next\n",
    "\n",
    "There are two major limitations that hinder the current OCR/Document Parser + VLM embedding:\n",
    "\n",
    "1. **Speed of Embedding**: The process of embedding can be time-consuming, affecting the overall efficiency.\n",
    "2. **Dirty Output Format**: Current data extraction tools often produce unclean outputs, particularly for automatic schema inference tasks with multimodal documentation.\n",
    "\n",
    "The ideal solution would involve extracting images as a whole, including their legends, and also being able to extract vector graphic images from PDFs, which current parsing tools fail to do. It is surprising how much the formats of these multimodal documents vary, making it nearly impossible to enforce consistent rules on these parsers to make them work.\n",
    "\n",
    "This makes the new [Colpali](https://huggingface.co/blog/manu/colpali) model an interesting option to explore. Colpali takes a complete vision approach to embedding unstructured data by solely using the image representation of document pages.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
